{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Early Stopping.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rFv5L_TrEQtz"},"source":["# [Reference](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n","\n","# Create Dataset\n","# Create Model"]},{"cell_type":"code","metadata":{"id":"Kx3Pi2BgWdlQ","executionInfo":{"status":"ok","timestamp":1620194660229,"user_tz":-360,"elapsed":1274,"user":{"displayName":"Haisam Rafid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCFh6znq8o7cMmaOaXWi0DlP9pXtcctfquM4PxnA=s64","userId":"02589669764555967414"}}},"source":["import pandas as pd\n","\n","from sklearn.datasets import make_moons\n","\n","X, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.utils.vis_utils import plot_model\n","\n","model = Sequential()\n","\n","hidden_layer1 = Dense(500, input_dim=2, activation='relu')  # relu = Rectified Linear Unit\n","model.add(hidden_layer1)\n","\n","output_layer = Dense(1, activation='sigmoid')\n","model.add(output_layer)\n","\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam')"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sl5UyTuNEUVQ"},"source":["# Train Test Split"]},{"cell_type":"code","metadata":{"id":"uetUbiqoXVqj","executionInfo":{"status":"ok","timestamp":1620194663837,"user_tz":-360,"elapsed":2120,"user":{"displayName":"Haisam Rafid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCFh6znq8o7cMmaOaXWi0DlP9pXtcctfquM4PxnA=s64","userId":"02589669764555967414"}}},"source":["n_train = 30\n","train_x, test_x = X[:n_train, :], X[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ovt_mEypEWC5"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"w8kf083XXYrS","executionInfo":{"status":"ok","timestamp":1620194740640,"user_tz":-360,"elapsed":75302,"user":{"displayName":"Haisam Rafid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCFh6znq8o7cMmaOaXWi0DlP9pXtcctfquM4PxnA=s64","userId":"02589669764555967414"}},"outputId":"905beb65-6087-45e0-e292-9608570d75cc"},"source":["import matplotlib.pyplot as plt\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","import pickle as pkl\n","\n","# es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1)\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n","mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n","\n","history = model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=2000, callbacks=[es, mc])\n","\n","model_json = model.to_json()\n","f = open('model.json', 'w')\n","f.write(model_json)\n","f.close()\n","\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.title('model performance')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend()\n","plt.show()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Epoch 1/2000\n","1/1 [==============================] - 1s 580ms/step - loss: 0.6659 - val_loss: 0.6645\n","\n","Epoch 00001: val_loss improved from inf to 0.66453, saving model to best_model.h5\n","Epoch 2/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.6499 - val_loss: 0.6546\n","\n","Epoch 00002: val_loss improved from 0.66453 to 0.65456, saving model to best_model.h5\n","Epoch 3/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.6343 - val_loss: 0.6449\n","\n","Epoch 00003: val_loss improved from 0.65456 to 0.64494, saving model to best_model.h5\n","Epoch 4/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.6192 - val_loss: 0.6357\n","\n","Epoch 00004: val_loss improved from 0.64494 to 0.63566, saving model to best_model.h5\n","Epoch 5/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.6046 - val_loss: 0.6267\n","\n","Epoch 00005: val_loss improved from 0.63566 to 0.62674, saving model to best_model.h5\n","Epoch 6/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.5904 - val_loss: 0.6182\n","\n","Epoch 00006: val_loss improved from 0.62674 to 0.61817, saving model to best_model.h5\n","Epoch 7/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.5766 - val_loss: 0.6099\n","\n","Epoch 00007: val_loss improved from 0.61817 to 0.60993, saving model to best_model.h5\n","Epoch 8/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.5633 - val_loss: 0.6020\n","\n","Epoch 00008: val_loss improved from 0.60993 to 0.60199, saving model to best_model.h5\n","Epoch 9/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.5504 - val_loss: 0.5943\n","\n","Epoch 00009: val_loss improved from 0.60199 to 0.59435, saving model to best_model.h5\n","Epoch 10/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.5378 - val_loss: 0.5870\n","\n","Epoch 00010: val_loss improved from 0.59435 to 0.58700, saving model to best_model.h5\n","Epoch 11/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.5257 - val_loss: 0.5799\n","\n","Epoch 00011: val_loss improved from 0.58700 to 0.57991, saving model to best_model.h5\n","Epoch 12/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.5139 - val_loss: 0.5731\n","\n","Epoch 00012: val_loss improved from 0.57991 to 0.57309, saving model to best_model.h5\n","Epoch 13/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.5024 - val_loss: 0.5665\n","\n","Epoch 00013: val_loss improved from 0.57309 to 0.56654, saving model to best_model.h5\n","Epoch 14/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.4913 - val_loss: 0.5602\n","\n","Epoch 00014: val_loss improved from 0.56654 to 0.56025, saving model to best_model.h5\n","Epoch 15/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.4805 - val_loss: 0.5542\n","\n","Epoch 00015: val_loss improved from 0.56025 to 0.55421, saving model to best_model.h5\n","Epoch 16/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.4701 - val_loss: 0.5484\n","\n","Epoch 00016: val_loss improved from 0.55421 to 0.54842, saving model to best_model.h5\n","Epoch 17/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.4599 - val_loss: 0.5429\n","\n","Epoch 00017: val_loss improved from 0.54842 to 0.54287, saving model to best_model.h5\n","Epoch 18/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.4501 - val_loss: 0.5375\n","\n","Epoch 00018: val_loss improved from 0.54287 to 0.53754, saving model to best_model.h5\n","Epoch 19/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.4405 - val_loss: 0.5324\n","\n","Epoch 00019: val_loss improved from 0.53754 to 0.53243, saving model to best_model.h5\n","Epoch 20/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.4312 - val_loss: 0.5275\n","\n","Epoch 00020: val_loss improved from 0.53243 to 0.52754, saving model to best_model.h5\n","Epoch 21/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.4222 - val_loss: 0.5228\n","\n","Epoch 00021: val_loss improved from 0.52754 to 0.52284, saving model to best_model.h5\n","Epoch 22/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.4134 - val_loss: 0.5184\n","\n","Epoch 00022: val_loss improved from 0.52284 to 0.51836, saving model to best_model.h5\n","Epoch 23/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.4049 - val_loss: 0.5141\n","\n","Epoch 00023: val_loss improved from 0.51836 to 0.51406, saving model to best_model.h5\n","Epoch 24/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.3966 - val_loss: 0.5099\n","\n","Epoch 00024: val_loss improved from 0.51406 to 0.50995, saving model to best_model.h5\n","Epoch 25/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.3886 - val_loss: 0.5060\n","\n","Epoch 00025: val_loss improved from 0.50995 to 0.50601, saving model to best_model.h5\n","Epoch 26/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.3809 - val_loss: 0.5023\n","\n","Epoch 00026: val_loss improved from 0.50601 to 0.50227, saving model to best_model.h5\n","Epoch 27/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.3733 - val_loss: 0.4987\n","\n","Epoch 00027: val_loss improved from 0.50227 to 0.49871, saving model to best_model.h5\n","Epoch 28/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3661 - val_loss: 0.4953\n","\n","Epoch 00028: val_loss improved from 0.49871 to 0.49534, saving model to best_model.h5\n","Epoch 29/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.3590 - val_loss: 0.4921\n","\n","Epoch 00029: val_loss improved from 0.49534 to 0.49213, saving model to best_model.h5\n","Epoch 30/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.3523 - val_loss: 0.4891\n","\n","Epoch 00030: val_loss improved from 0.49213 to 0.48910, saving model to best_model.h5\n","Epoch 31/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.3457 - val_loss: 0.4862\n","\n","Epoch 00031: val_loss improved from 0.48910 to 0.48622, saving model to best_model.h5\n","Epoch 32/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.3393 - val_loss: 0.4835\n","\n","Epoch 00032: val_loss improved from 0.48622 to 0.48350, saving model to best_model.h5\n","Epoch 33/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.3332 - val_loss: 0.4809\n","\n","Epoch 00033: val_loss improved from 0.48350 to 0.48093, saving model to best_model.h5\n","Epoch 34/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.3273 - val_loss: 0.4785\n","\n","Epoch 00034: val_loss improved from 0.48093 to 0.47851, saving model to best_model.h5\n","Epoch 35/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.3216 - val_loss: 0.4762\n","\n","Epoch 00035: val_loss improved from 0.47851 to 0.47623, saving model to best_model.h5\n","Epoch 36/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.3160 - val_loss: 0.4741\n","\n","Epoch 00036: val_loss improved from 0.47623 to 0.47409, saving model to best_model.h5\n","Epoch 37/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.3107 - val_loss: 0.4721\n","\n","Epoch 00037: val_loss improved from 0.47409 to 0.47207, saving model to best_model.h5\n","Epoch 38/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.3056 - val_loss: 0.4702\n","\n","Epoch 00038: val_loss improved from 0.47207 to 0.47018, saving model to best_model.h5\n","Epoch 39/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.3007 - val_loss: 0.4684\n","\n","Epoch 00039: val_loss improved from 0.47018 to 0.46841, saving model to best_model.h5\n","Epoch 40/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.2959 - val_loss: 0.4668\n","\n","Epoch 00040: val_loss improved from 0.46841 to 0.46677, saving model to best_model.h5\n","Epoch 41/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.2914 - val_loss: 0.4652\n","\n","Epoch 00041: val_loss improved from 0.46677 to 0.46523, saving model to best_model.h5\n","Epoch 42/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2870 - val_loss: 0.4638\n","\n","Epoch 00042: val_loss improved from 0.46523 to 0.46381, saving model to best_model.h5\n","Epoch 43/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2828 - val_loss: 0.4625\n","\n","Epoch 00043: val_loss improved from 0.46381 to 0.46249, saving model to best_model.h5\n","Epoch 44/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2788 - val_loss: 0.4613\n","\n","Epoch 00044: val_loss improved from 0.46249 to 0.46126, saving model to best_model.h5\n","Epoch 45/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2749 - val_loss: 0.4601\n","\n","Epoch 00045: val_loss improved from 0.46126 to 0.46011, saving model to best_model.h5\n","Epoch 46/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2712 - val_loss: 0.4590\n","\n","Epoch 00046: val_loss improved from 0.46011 to 0.45905, saving model to best_model.h5\n","Epoch 47/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2676 - val_loss: 0.4581\n","\n","Epoch 00047: val_loss improved from 0.45905 to 0.45806, saving model to best_model.h5\n","Epoch 48/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2642 - val_loss: 0.4571\n","\n","Epoch 00048: val_loss improved from 0.45806 to 0.45713, saving model to best_model.h5\n","Epoch 49/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2609 - val_loss: 0.4563\n","\n","Epoch 00049: val_loss improved from 0.45713 to 0.45627, saving model to best_model.h5\n","Epoch 50/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.2578 - val_loss: 0.4555\n","\n","Epoch 00050: val_loss improved from 0.45627 to 0.45545, saving model to best_model.h5\n","Epoch 51/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2548 - val_loss: 0.4547\n","\n","Epoch 00051: val_loss improved from 0.45545 to 0.45469, saving model to best_model.h5\n","Epoch 52/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2519 - val_loss: 0.4540\n","\n","Epoch 00052: val_loss improved from 0.45469 to 0.45397, saving model to best_model.h5\n","Epoch 53/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.2491 - val_loss: 0.4533\n","\n","Epoch 00053: val_loss improved from 0.45397 to 0.45329, saving model to best_model.h5\n","Epoch 54/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2464 - val_loss: 0.4526\n","\n","Epoch 00054: val_loss improved from 0.45329 to 0.45262, saving model to best_model.h5\n","Epoch 55/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2439 - val_loss: 0.4520\n","\n","Epoch 00055: val_loss improved from 0.45262 to 0.45198, saving model to best_model.h5\n","Epoch 56/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.2414 - val_loss: 0.4514\n","\n","Epoch 00056: val_loss improved from 0.45198 to 0.45136, saving model to best_model.h5\n","Epoch 57/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2390 - val_loss: 0.4507\n","\n","Epoch 00057: val_loss improved from 0.45136 to 0.45075, saving model to best_model.h5\n","Epoch 58/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2367 - val_loss: 0.4501\n","\n","Epoch 00058: val_loss improved from 0.45075 to 0.45015, saving model to best_model.h5\n","Epoch 59/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.2345 - val_loss: 0.4496\n","\n","Epoch 00059: val_loss improved from 0.45015 to 0.44956, saving model to best_model.h5\n","Epoch 60/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2324 - val_loss: 0.4490\n","\n","Epoch 00060: val_loss improved from 0.44956 to 0.44898, saving model to best_model.h5\n","Epoch 61/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2304 - val_loss: 0.4484\n","\n","Epoch 00061: val_loss improved from 0.44898 to 0.44842, saving model to best_model.h5\n","Epoch 62/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2284 - val_loss: 0.4479\n","\n","Epoch 00062: val_loss improved from 0.44842 to 0.44787, saving model to best_model.h5\n","Epoch 63/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2266 - val_loss: 0.4473\n","\n","Epoch 00063: val_loss improved from 0.44787 to 0.44733, saving model to best_model.h5\n","Epoch 64/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2248 - val_loss: 0.4468\n","\n","Epoch 00064: val_loss improved from 0.44733 to 0.44679, saving model to best_model.h5\n","Epoch 65/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.2230 - val_loss: 0.4463\n","\n","Epoch 00065: val_loss improved from 0.44679 to 0.44626, saving model to best_model.h5\n","Epoch 66/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2214 - val_loss: 0.4457\n","\n","Epoch 00066: val_loss improved from 0.44626 to 0.44573, saving model to best_model.h5\n","Epoch 67/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2197 - val_loss: 0.4452\n","\n","Epoch 00067: val_loss improved from 0.44573 to 0.44521, saving model to best_model.h5\n","Epoch 68/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.2182 - val_loss: 0.4447\n","\n","Epoch 00068: val_loss improved from 0.44521 to 0.44469, saving model to best_model.h5\n","Epoch 69/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.2167 - val_loss: 0.4442\n","\n","Epoch 00069: val_loss improved from 0.44469 to 0.44416, saving model to best_model.h5\n","Epoch 70/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2152 - val_loss: 0.4436\n","\n","Epoch 00070: val_loss improved from 0.44416 to 0.44363, saving model to best_model.h5\n","Epoch 71/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2138 - val_loss: 0.4431\n","\n","Epoch 00071: val_loss improved from 0.44363 to 0.44310, saving model to best_model.h5\n","Epoch 72/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2124 - val_loss: 0.4426\n","\n","Epoch 00072: val_loss improved from 0.44310 to 0.44256, saving model to best_model.h5\n","Epoch 73/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.2111 - val_loss: 0.4420\n","\n","Epoch 00073: val_loss improved from 0.44256 to 0.44202, saving model to best_model.h5\n","Epoch 74/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2098 - val_loss: 0.4415\n","\n","Epoch 00074: val_loss improved from 0.44202 to 0.44148, saving model to best_model.h5\n","Epoch 75/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.2086 - val_loss: 0.4409\n","\n","Epoch 00075: val_loss improved from 0.44148 to 0.44094, saving model to best_model.h5\n","Epoch 76/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2074 - val_loss: 0.4404\n","\n","Epoch 00076: val_loss improved from 0.44094 to 0.44040, saving model to best_model.h5\n","Epoch 77/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.2062 - val_loss: 0.4398\n","\n","Epoch 00077: val_loss improved from 0.44040 to 0.43985, saving model to best_model.h5\n","Epoch 78/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2051 - val_loss: 0.4393\n","\n","Epoch 00078: val_loss improved from 0.43985 to 0.43930, saving model to best_model.h5\n","Epoch 79/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.2040 - val_loss: 0.4388\n","\n","Epoch 00079: val_loss improved from 0.43930 to 0.43876, saving model to best_model.h5\n","Epoch 80/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2029 - val_loss: 0.4382\n","\n","Epoch 00080: val_loss improved from 0.43876 to 0.43822, saving model to best_model.h5\n","Epoch 81/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2019 - val_loss: 0.4377\n","\n","Epoch 00081: val_loss improved from 0.43822 to 0.43768, saving model to best_model.h5\n","Epoch 82/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2009 - val_loss: 0.4371\n","\n","Epoch 00082: val_loss improved from 0.43768 to 0.43714, saving model to best_model.h5\n","Epoch 83/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1999 - val_loss: 0.4366\n","\n","Epoch 00083: val_loss improved from 0.43714 to 0.43661, saving model to best_model.h5\n","Epoch 84/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1989 - val_loss: 0.4361\n","\n","Epoch 00084: val_loss improved from 0.43661 to 0.43607, saving model to best_model.h5\n","Epoch 85/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1980 - val_loss: 0.4355\n","\n","Epoch 00085: val_loss improved from 0.43607 to 0.43552, saving model to best_model.h5\n","Epoch 86/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1970 - val_loss: 0.4350\n","\n","Epoch 00086: val_loss improved from 0.43552 to 0.43497, saving model to best_model.h5\n","Epoch 87/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1961 - val_loss: 0.4344\n","\n","Epoch 00087: val_loss improved from 0.43497 to 0.43440, saving model to best_model.h5\n","Epoch 88/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1953 - val_loss: 0.4338\n","\n","Epoch 00088: val_loss improved from 0.43440 to 0.43383, saving model to best_model.h5\n","Epoch 89/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1944 - val_loss: 0.4332\n","\n","Epoch 00089: val_loss improved from 0.43383 to 0.43324, saving model to best_model.h5\n","Epoch 90/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1935 - val_loss: 0.4326\n","\n","Epoch 00090: val_loss improved from 0.43324 to 0.43264, saving model to best_model.h5\n","Epoch 91/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1927 - val_loss: 0.4320\n","\n","Epoch 00091: val_loss improved from 0.43264 to 0.43202, saving model to best_model.h5\n","Epoch 92/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1919 - val_loss: 0.4314\n","\n","Epoch 00092: val_loss improved from 0.43202 to 0.43138, saving model to best_model.h5\n","Epoch 93/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1911 - val_loss: 0.4307\n","\n","Epoch 00093: val_loss improved from 0.43138 to 0.43074, saving model to best_model.h5\n","Epoch 94/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1903 - val_loss: 0.4301\n","\n","Epoch 00094: val_loss improved from 0.43074 to 0.43008, saving model to best_model.h5\n","Epoch 95/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1895 - val_loss: 0.4294\n","\n","Epoch 00095: val_loss improved from 0.43008 to 0.42941, saving model to best_model.h5\n","Epoch 96/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1888 - val_loss: 0.4287\n","\n","Epoch 00096: val_loss improved from 0.42941 to 0.42873, saving model to best_model.h5\n","Epoch 97/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1880 - val_loss: 0.4280\n","\n","Epoch 00097: val_loss improved from 0.42873 to 0.42803, saving model to best_model.h5\n","Epoch 98/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1873 - val_loss: 0.4273\n","\n","Epoch 00098: val_loss improved from 0.42803 to 0.42732, saving model to best_model.h5\n","Epoch 99/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1866 - val_loss: 0.4266\n","\n","Epoch 00099: val_loss improved from 0.42732 to 0.42660, saving model to best_model.h5\n","Epoch 100/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1858 - val_loss: 0.4259\n","\n","Epoch 00100: val_loss improved from 0.42660 to 0.42588, saving model to best_model.h5\n","Epoch 101/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1852 - val_loss: 0.4251\n","\n","Epoch 00101: val_loss improved from 0.42588 to 0.42514, saving model to best_model.h5\n","Epoch 102/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1845 - val_loss: 0.4244\n","\n","Epoch 00102: val_loss improved from 0.42514 to 0.42440, saving model to best_model.h5\n","Epoch 103/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1838 - val_loss: 0.4236\n","\n","Epoch 00103: val_loss improved from 0.42440 to 0.42365, saving model to best_model.h5\n","Epoch 104/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1831 - val_loss: 0.4229\n","\n","Epoch 00104: val_loss improved from 0.42365 to 0.42289, saving model to best_model.h5\n","Epoch 105/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1824 - val_loss: 0.4221\n","\n","Epoch 00105: val_loss improved from 0.42289 to 0.42212, saving model to best_model.h5\n","Epoch 106/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1818 - val_loss: 0.4214\n","\n","Epoch 00106: val_loss improved from 0.42212 to 0.42135, saving model to best_model.h5\n","Epoch 107/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1811 - val_loss: 0.4206\n","\n","Epoch 00107: val_loss improved from 0.42135 to 0.42058, saving model to best_model.h5\n","Epoch 108/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1805 - val_loss: 0.4198\n","\n","Epoch 00108: val_loss improved from 0.42058 to 0.41979, saving model to best_model.h5\n","Epoch 109/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1799 - val_loss: 0.4190\n","\n","Epoch 00109: val_loss improved from 0.41979 to 0.41900, saving model to best_model.h5\n","Epoch 110/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1792 - val_loss: 0.4182\n","\n","Epoch 00110: val_loss improved from 0.41900 to 0.41820, saving model to best_model.h5\n","Epoch 111/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1786 - val_loss: 0.4174\n","\n","Epoch 00111: val_loss improved from 0.41820 to 0.41740, saving model to best_model.h5\n","Epoch 112/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1780 - val_loss: 0.4166\n","\n","Epoch 00112: val_loss improved from 0.41740 to 0.41660, saving model to best_model.h5\n","Epoch 113/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1774 - val_loss: 0.4158\n","\n","Epoch 00113: val_loss improved from 0.41660 to 0.41580, saving model to best_model.h5\n","Epoch 114/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1768 - val_loss: 0.4150\n","\n","Epoch 00114: val_loss improved from 0.41580 to 0.41500, saving model to best_model.h5\n","Epoch 115/2000\n","1/1 [==============================] - 0s 41ms/step - loss: 0.1762 - val_loss: 0.4142\n","\n","Epoch 00115: val_loss improved from 0.41500 to 0.41419, saving model to best_model.h5\n","Epoch 116/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1756 - val_loss: 0.4134\n","\n","Epoch 00116: val_loss improved from 0.41419 to 0.41338, saving model to best_model.h5\n","Epoch 117/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1750 - val_loss: 0.4126\n","\n","Epoch 00117: val_loss improved from 0.41338 to 0.41257, saving model to best_model.h5\n","Epoch 118/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1745 - val_loss: 0.4118\n","\n","Epoch 00118: val_loss improved from 0.41257 to 0.41175, saving model to best_model.h5\n","Epoch 119/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1739 - val_loss: 0.4109\n","\n","Epoch 00119: val_loss improved from 0.41175 to 0.41093, saving model to best_model.h5\n","Epoch 120/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1733 - val_loss: 0.4101\n","\n","Epoch 00120: val_loss improved from 0.41093 to 0.41012, saving model to best_model.h5\n","Epoch 121/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1728 - val_loss: 0.4093\n","\n","Epoch 00121: val_loss improved from 0.41012 to 0.40932, saving model to best_model.h5\n","Epoch 122/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1722 - val_loss: 0.4085\n","\n","Epoch 00122: val_loss improved from 0.40932 to 0.40853, saving model to best_model.h5\n","Epoch 123/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1717 - val_loss: 0.4077\n","\n","Epoch 00123: val_loss improved from 0.40853 to 0.40774, saving model to best_model.h5\n","Epoch 124/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1711 - val_loss: 0.4070\n","\n","Epoch 00124: val_loss improved from 0.40774 to 0.40696, saving model to best_model.h5\n","Epoch 125/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1706 - val_loss: 0.4062\n","\n","Epoch 00125: val_loss improved from 0.40696 to 0.40617, saving model to best_model.h5\n","Epoch 126/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1701 - val_loss: 0.4054\n","\n","Epoch 00126: val_loss improved from 0.40617 to 0.40538, saving model to best_model.h5\n","Epoch 127/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1696 - val_loss: 0.4046\n","\n","Epoch 00127: val_loss improved from 0.40538 to 0.40458, saving model to best_model.h5\n","Epoch 128/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1690 - val_loss: 0.4038\n","\n","Epoch 00128: val_loss improved from 0.40458 to 0.40378, saving model to best_model.h5\n","Epoch 129/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1685 - val_loss: 0.4030\n","\n","Epoch 00129: val_loss improved from 0.40378 to 0.40299, saving model to best_model.h5\n","Epoch 130/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1680 - val_loss: 0.4022\n","\n","Epoch 00130: val_loss improved from 0.40299 to 0.40222, saving model to best_model.h5\n","Epoch 131/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1675 - val_loss: 0.4015\n","\n","Epoch 00131: val_loss improved from 0.40222 to 0.40145, saving model to best_model.h5\n","Epoch 132/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1670 - val_loss: 0.4007\n","\n","Epoch 00132: val_loss improved from 0.40145 to 0.40069, saving model to best_model.h5\n","Epoch 133/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1665 - val_loss: 0.3999\n","\n","Epoch 00133: val_loss improved from 0.40069 to 0.39993, saving model to best_model.h5\n","Epoch 134/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1660 - val_loss: 0.3992\n","\n","Epoch 00134: val_loss improved from 0.39993 to 0.39918, saving model to best_model.h5\n","Epoch 135/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1655 - val_loss: 0.3984\n","\n","Epoch 00135: val_loss improved from 0.39918 to 0.39845, saving model to best_model.h5\n","Epoch 136/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1651 - val_loss: 0.3977\n","\n","Epoch 00136: val_loss improved from 0.39845 to 0.39772, saving model to best_model.h5\n","Epoch 137/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1646 - val_loss: 0.3970\n","\n","Epoch 00137: val_loss improved from 0.39772 to 0.39699, saving model to best_model.h5\n","Epoch 138/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1641 - val_loss: 0.3963\n","\n","Epoch 00138: val_loss improved from 0.39699 to 0.39626, saving model to best_model.h5\n","Epoch 139/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1636 - val_loss: 0.3956\n","\n","Epoch 00139: val_loss improved from 0.39626 to 0.39555, saving model to best_model.h5\n","Epoch 140/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1632 - val_loss: 0.3949\n","\n","Epoch 00140: val_loss improved from 0.39555 to 0.39485, saving model to best_model.h5\n","Epoch 141/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1627 - val_loss: 0.3942\n","\n","Epoch 00141: val_loss improved from 0.39485 to 0.39416, saving model to best_model.h5\n","Epoch 142/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1623 - val_loss: 0.3935\n","\n","Epoch 00142: val_loss improved from 0.39416 to 0.39346, saving model to best_model.h5\n","Epoch 143/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1618 - val_loss: 0.3928\n","\n","Epoch 00143: val_loss improved from 0.39346 to 0.39277, saving model to best_model.h5\n","Epoch 144/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1614 - val_loss: 0.3921\n","\n","Epoch 00144: val_loss improved from 0.39277 to 0.39208, saving model to best_model.h5\n","Epoch 145/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1609 - val_loss: 0.3914\n","\n","Epoch 00145: val_loss improved from 0.39208 to 0.39140, saving model to best_model.h5\n","Epoch 146/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1605 - val_loss: 0.3907\n","\n","Epoch 00146: val_loss improved from 0.39140 to 0.39072, saving model to best_model.h5\n","Epoch 147/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1601 - val_loss: 0.3900\n","\n","Epoch 00147: val_loss improved from 0.39072 to 0.39005, saving model to best_model.h5\n","Epoch 148/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1597 - val_loss: 0.3894\n","\n","Epoch 00148: val_loss improved from 0.39005 to 0.38938, saving model to best_model.h5\n","Epoch 149/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1592 - val_loss: 0.3887\n","\n","Epoch 00149: val_loss improved from 0.38938 to 0.38873, saving model to best_model.h5\n","Epoch 150/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1588 - val_loss: 0.3881\n","\n","Epoch 00150: val_loss improved from 0.38873 to 0.38809, saving model to best_model.h5\n","Epoch 151/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1584 - val_loss: 0.3875\n","\n","Epoch 00151: val_loss improved from 0.38809 to 0.38745, saving model to best_model.h5\n","Epoch 152/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1580 - val_loss: 0.3868\n","\n","Epoch 00152: val_loss improved from 0.38745 to 0.38684, saving model to best_model.h5\n","Epoch 153/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1576 - val_loss: 0.3862\n","\n","Epoch 00153: val_loss improved from 0.38684 to 0.38623, saving model to best_model.h5\n","Epoch 154/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1572 - val_loss: 0.3856\n","\n","Epoch 00154: val_loss improved from 0.38623 to 0.38563, saving model to best_model.h5\n","Epoch 155/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1569 - val_loss: 0.3850\n","\n","Epoch 00155: val_loss improved from 0.38563 to 0.38503, saving model to best_model.h5\n","Epoch 156/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1565 - val_loss: 0.3845\n","\n","Epoch 00156: val_loss improved from 0.38503 to 0.38446, saving model to best_model.h5\n","Epoch 157/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1561 - val_loss: 0.3839\n","\n","Epoch 00157: val_loss improved from 0.38446 to 0.38389, saving model to best_model.h5\n","Epoch 158/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1557 - val_loss: 0.3833\n","\n","Epoch 00158: val_loss improved from 0.38389 to 0.38333, saving model to best_model.h5\n","Epoch 159/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1554 - val_loss: 0.3828\n","\n","Epoch 00159: val_loss improved from 0.38333 to 0.38277, saving model to best_model.h5\n","Epoch 160/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1550 - val_loss: 0.3822\n","\n","Epoch 00160: val_loss improved from 0.38277 to 0.38222, saving model to best_model.h5\n","Epoch 161/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1546 - val_loss: 0.3817\n","\n","Epoch 00161: val_loss improved from 0.38222 to 0.38168, saving model to best_model.h5\n","Epoch 162/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1543 - val_loss: 0.3812\n","\n","Epoch 00162: val_loss improved from 0.38168 to 0.38115, saving model to best_model.h5\n","Epoch 163/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1539 - val_loss: 0.3807\n","\n","Epoch 00163: val_loss improved from 0.38115 to 0.38065, saving model to best_model.h5\n","Epoch 164/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1536 - val_loss: 0.3802\n","\n","Epoch 00164: val_loss improved from 0.38065 to 0.38018, saving model to best_model.h5\n","Epoch 165/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1532 - val_loss: 0.3797\n","\n","Epoch 00165: val_loss improved from 0.38018 to 0.37972, saving model to best_model.h5\n","Epoch 166/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1529 - val_loss: 0.3793\n","\n","Epoch 00166: val_loss improved from 0.37972 to 0.37927, saving model to best_model.h5\n","Epoch 167/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1526 - val_loss: 0.3788\n","\n","Epoch 00167: val_loss improved from 0.37927 to 0.37882, saving model to best_model.h5\n","Epoch 168/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1522 - val_loss: 0.3784\n","\n","Epoch 00168: val_loss improved from 0.37882 to 0.37837, saving model to best_model.h5\n","Epoch 169/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1519 - val_loss: 0.3779\n","\n","Epoch 00169: val_loss improved from 0.37837 to 0.37795, saving model to best_model.h5\n","Epoch 170/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1516 - val_loss: 0.3775\n","\n","Epoch 00170: val_loss improved from 0.37795 to 0.37753, saving model to best_model.h5\n","Epoch 171/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1513 - val_loss: 0.3771\n","\n","Epoch 00171: val_loss improved from 0.37753 to 0.37713, saving model to best_model.h5\n","Epoch 172/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1510 - val_loss: 0.3767\n","\n","Epoch 00172: val_loss improved from 0.37713 to 0.37674, saving model to best_model.h5\n","Epoch 173/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1507 - val_loss: 0.3763\n","\n","Epoch 00173: val_loss improved from 0.37674 to 0.37634, saving model to best_model.h5\n","Epoch 174/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1503 - val_loss: 0.3760\n","\n","Epoch 00174: val_loss improved from 0.37634 to 0.37596, saving model to best_model.h5\n","Epoch 175/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1501 - val_loss: 0.3756\n","\n","Epoch 00175: val_loss improved from 0.37596 to 0.37559, saving model to best_model.h5\n","Epoch 176/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1498 - val_loss: 0.3752\n","\n","Epoch 00176: val_loss improved from 0.37559 to 0.37524, saving model to best_model.h5\n","Epoch 177/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1495 - val_loss: 0.3749\n","\n","Epoch 00177: val_loss improved from 0.37524 to 0.37489, saving model to best_model.h5\n","Epoch 178/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1492 - val_loss: 0.3745\n","\n","Epoch 00178: val_loss improved from 0.37489 to 0.37454, saving model to best_model.h5\n","Epoch 179/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1489 - val_loss: 0.3742\n","\n","Epoch 00179: val_loss improved from 0.37454 to 0.37420, saving model to best_model.h5\n","Epoch 180/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1486 - val_loss: 0.3739\n","\n","Epoch 00180: val_loss improved from 0.37420 to 0.37389, saving model to best_model.h5\n","Epoch 181/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1483 - val_loss: 0.3736\n","\n","Epoch 00181: val_loss improved from 0.37389 to 0.37359, saving model to best_model.h5\n","Epoch 182/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1481 - val_loss: 0.3733\n","\n","Epoch 00182: val_loss improved from 0.37359 to 0.37329, saving model to best_model.h5\n","Epoch 183/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1478 - val_loss: 0.3730\n","\n","Epoch 00183: val_loss improved from 0.37329 to 0.37300, saving model to best_model.h5\n","Epoch 184/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1475 - val_loss: 0.3727\n","\n","Epoch 00184: val_loss improved from 0.37300 to 0.37271, saving model to best_model.h5\n","Epoch 185/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1473 - val_loss: 0.3724\n","\n","Epoch 00185: val_loss improved from 0.37271 to 0.37243, saving model to best_model.h5\n","Epoch 186/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1470 - val_loss: 0.3722\n","\n","Epoch 00186: val_loss improved from 0.37243 to 0.37217, saving model to best_model.h5\n","Epoch 187/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1468 - val_loss: 0.3719\n","\n","Epoch 00187: val_loss improved from 0.37217 to 0.37193, saving model to best_model.h5\n","Epoch 188/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1465 - val_loss: 0.3717\n","\n","Epoch 00188: val_loss improved from 0.37193 to 0.37170, saving model to best_model.h5\n","Epoch 189/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1463 - val_loss: 0.3715\n","\n","Epoch 00189: val_loss improved from 0.37170 to 0.37146, saving model to best_model.h5\n","Epoch 190/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1461 - val_loss: 0.3713\n","\n","Epoch 00190: val_loss improved from 0.37146 to 0.37125, saving model to best_model.h5\n","Epoch 191/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1458 - val_loss: 0.3710\n","\n","Epoch 00191: val_loss improved from 0.37125 to 0.37103, saving model to best_model.h5\n","Epoch 192/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1456 - val_loss: 0.3708\n","\n","Epoch 00192: val_loss improved from 0.37103 to 0.37081, saving model to best_model.h5\n","Epoch 193/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1454 - val_loss: 0.3706\n","\n","Epoch 00193: val_loss improved from 0.37081 to 0.37061, saving model to best_model.h5\n","Epoch 194/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1452 - val_loss: 0.3704\n","\n","Epoch 00194: val_loss improved from 0.37061 to 0.37043, saving model to best_model.h5\n","Epoch 195/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1449 - val_loss: 0.3703\n","\n","Epoch 00195: val_loss improved from 0.37043 to 0.37026, saving model to best_model.h5\n","Epoch 196/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1447 - val_loss: 0.3701\n","\n","Epoch 00196: val_loss improved from 0.37026 to 0.37011, saving model to best_model.h5\n","Epoch 197/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1445 - val_loss: 0.3700\n","\n","Epoch 00197: val_loss improved from 0.37011 to 0.36997, saving model to best_model.h5\n","Epoch 198/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1443 - val_loss: 0.3698\n","\n","Epoch 00198: val_loss improved from 0.36997 to 0.36984, saving model to best_model.h5\n","Epoch 199/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1441 - val_loss: 0.3697\n","\n","Epoch 00199: val_loss improved from 0.36984 to 0.36972, saving model to best_model.h5\n","Epoch 200/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1439 - val_loss: 0.3696\n","\n","Epoch 00200: val_loss improved from 0.36972 to 0.36960, saving model to best_model.h5\n","Epoch 201/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1437 - val_loss: 0.3695\n","\n","Epoch 00201: val_loss improved from 0.36960 to 0.36947, saving model to best_model.h5\n","Epoch 202/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1435 - val_loss: 0.3694\n","\n","Epoch 00202: val_loss improved from 0.36947 to 0.36936, saving model to best_model.h5\n","Epoch 203/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1434 - val_loss: 0.3693\n","\n","Epoch 00203: val_loss improved from 0.36936 to 0.36926, saving model to best_model.h5\n","Epoch 204/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1432 - val_loss: 0.3692\n","\n","Epoch 00204: val_loss improved from 0.36926 to 0.36919, saving model to best_model.h5\n","Epoch 205/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1430 - val_loss: 0.3691\n","\n","Epoch 00205: val_loss improved from 0.36919 to 0.36914, saving model to best_model.h5\n","Epoch 206/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1428 - val_loss: 0.3691\n","\n","Epoch 00206: val_loss improved from 0.36914 to 0.36910, saving model to best_model.h5\n","Epoch 207/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1426 - val_loss: 0.3691\n","\n","Epoch 00207: val_loss improved from 0.36910 to 0.36906, saving model to best_model.h5\n","Epoch 208/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1425 - val_loss: 0.3690\n","\n","Epoch 00208: val_loss improved from 0.36906 to 0.36901, saving model to best_model.h5\n","Epoch 209/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1423 - val_loss: 0.3690\n","\n","Epoch 00209: val_loss improved from 0.36901 to 0.36897, saving model to best_model.h5\n","Epoch 210/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1421 - val_loss: 0.3689\n","\n","Epoch 00210: val_loss improved from 0.36897 to 0.36894, saving model to best_model.h5\n","Epoch 211/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1420 - val_loss: 0.3689\n","\n","Epoch 00211: val_loss improved from 0.36894 to 0.36893, saving model to best_model.h5\n","Epoch 212/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1418 - val_loss: 0.3689\n","\n","Epoch 00212: val_loss improved from 0.36893 to 0.36890, saving model to best_model.h5\n","Epoch 213/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1416 - val_loss: 0.3689\n","\n","Epoch 00213: val_loss improved from 0.36890 to 0.36887, saving model to best_model.h5\n","Epoch 214/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1415 - val_loss: 0.3688\n","\n","Epoch 00214: val_loss improved from 0.36887 to 0.36885, saving model to best_model.h5\n","Epoch 215/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1413 - val_loss: 0.3688\n","\n","Epoch 00215: val_loss improved from 0.36885 to 0.36883, saving model to best_model.h5\n","Epoch 216/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1412 - val_loss: 0.3688\n","\n","Epoch 00216: val_loss improved from 0.36883 to 0.36883, saving model to best_model.h5\n","Epoch 217/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1410 - val_loss: 0.3688\n","\n","Epoch 00217: val_loss did not improve from 0.36883\n","Epoch 218/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1409 - val_loss: 0.3688\n","\n","Epoch 00218: val_loss did not improve from 0.36883\n","Epoch 219/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1407 - val_loss: 0.3688\n","\n","Epoch 00219: val_loss did not improve from 0.36883\n","Epoch 220/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1406 - val_loss: 0.3688\n","\n","Epoch 00220: val_loss improved from 0.36883 to 0.36882, saving model to best_model.h5\n","Epoch 221/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1404 - val_loss: 0.3688\n","\n","Epoch 00221: val_loss improved from 0.36882 to 0.36881, saving model to best_model.h5\n","Epoch 222/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1403 - val_loss: 0.3688\n","\n","Epoch 00222: val_loss improved from 0.36881 to 0.36880, saving model to best_model.h5\n","Epoch 223/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1401 - val_loss: 0.3688\n","\n","Epoch 00223: val_loss improved from 0.36880 to 0.36880, saving model to best_model.h5\n","Epoch 224/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1400 - val_loss: 0.3688\n","\n","Epoch 00224: val_loss did not improve from 0.36880\n","Epoch 225/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1398 - val_loss: 0.3688\n","\n","Epoch 00225: val_loss did not improve from 0.36880\n","Epoch 226/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1397 - val_loss: 0.3689\n","\n","Epoch 00226: val_loss did not improve from 0.36880\n","Epoch 227/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1396 - val_loss: 0.3689\n","\n","Epoch 00227: val_loss did not improve from 0.36880\n","Epoch 228/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1394 - val_loss: 0.3689\n","\n","Epoch 00228: val_loss did not improve from 0.36880\n","Epoch 229/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1393 - val_loss: 0.3689\n","\n","Epoch 00229: val_loss did not improve from 0.36880\n","Epoch 230/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1392 - val_loss: 0.3689\n","\n","Epoch 00230: val_loss did not improve from 0.36880\n","Epoch 231/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1390 - val_loss: 0.3689\n","\n","Epoch 00231: val_loss did not improve from 0.36880\n","Epoch 232/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1389 - val_loss: 0.3689\n","\n","Epoch 00232: val_loss did not improve from 0.36880\n","Epoch 233/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1388 - val_loss: 0.3689\n","\n","Epoch 00233: val_loss did not improve from 0.36880\n","Epoch 234/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1386 - val_loss: 0.3690\n","\n","Epoch 00234: val_loss did not improve from 0.36880\n","Epoch 235/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1385 - val_loss: 0.3690\n","\n","Epoch 00235: val_loss did not improve from 0.36880\n","Epoch 236/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1384 - val_loss: 0.3690\n","\n","Epoch 00236: val_loss did not improve from 0.36880\n","Epoch 237/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1383 - val_loss: 0.3690\n","\n","Epoch 00237: val_loss did not improve from 0.36880\n","Epoch 238/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1381 - val_loss: 0.3690\n","\n","Epoch 00238: val_loss did not improve from 0.36880\n","Epoch 239/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1380 - val_loss: 0.3690\n","\n","Epoch 00239: val_loss did not improve from 0.36880\n","Epoch 240/2000\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1379 - val_loss: 0.3690\n","\n","Epoch 00240: val_loss did not improve from 0.36880\n","Epoch 241/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1378 - val_loss: 0.3690\n","\n","Epoch 00241: val_loss did not improve from 0.36880\n","Epoch 242/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1376 - val_loss: 0.3690\n","\n","Epoch 00242: val_loss did not improve from 0.36880\n","Epoch 243/2000\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1375 - val_loss: 0.3690\n","\n","Epoch 00243: val_loss did not improve from 0.36880\n","Epoch 244/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1374 - val_loss: 0.3690\n","\n","Epoch 00244: val_loss did not improve from 0.36880\n","Epoch 245/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1373 - val_loss: 0.3690\n","\n","Epoch 00245: val_loss did not improve from 0.36880\n","Epoch 246/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1372 - val_loss: 0.3690\n","\n","Epoch 00246: val_loss did not improve from 0.36880\n","Epoch 247/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1370 - val_loss: 0.3690\n","\n","Epoch 00247: val_loss did not improve from 0.36880\n","Epoch 248/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1369 - val_loss: 0.3690\n","\n","Epoch 00248: val_loss did not improve from 0.36880\n","Epoch 249/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1368 - val_loss: 0.3689\n","\n","Epoch 00249: val_loss did not improve from 0.36880\n","Epoch 250/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1367 - val_loss: 0.3690\n","\n","Epoch 00250: val_loss did not improve from 0.36880\n","Epoch 251/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1366 - val_loss: 0.3690\n","\n","Epoch 00251: val_loss did not improve from 0.36880\n","Epoch 252/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1365 - val_loss: 0.3690\n","\n","Epoch 00252: val_loss did not improve from 0.36880\n","Epoch 253/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1364 - val_loss: 0.3690\n","\n","Epoch 00253: val_loss did not improve from 0.36880\n","Epoch 254/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1362 - val_loss: 0.3690\n","\n","Epoch 00254: val_loss did not improve from 0.36880\n","Epoch 255/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1361 - val_loss: 0.3690\n","\n","Epoch 00255: val_loss did not improve from 0.36880\n","Epoch 256/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1360 - val_loss: 0.3690\n","\n","Epoch 00256: val_loss did not improve from 0.36880\n","Epoch 257/2000\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1359 - val_loss: 0.3690\n","\n","Epoch 00257: val_loss did not improve from 0.36880\n","Epoch 258/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1358 - val_loss: 0.3690\n","\n","Epoch 00258: val_loss did not improve from 0.36880\n","Epoch 259/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1357 - val_loss: 0.3690\n","\n","Epoch 00259: val_loss did not improve from 0.36880\n","Epoch 260/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1356 - val_loss: 0.3690\n","\n","Epoch 00260: val_loss did not improve from 0.36880\n","Epoch 261/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1354 - val_loss: 0.3690\n","\n","Epoch 00261: val_loss did not improve from 0.36880\n","Epoch 262/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1353 - val_loss: 0.3690\n","\n","Epoch 00262: val_loss did not improve from 0.36880\n","Epoch 263/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1352 - val_loss: 0.3690\n","\n","Epoch 00263: val_loss did not improve from 0.36880\n","Epoch 264/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1351 - val_loss: 0.3690\n","\n","Epoch 00264: val_loss did not improve from 0.36880\n","Epoch 265/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1350 - val_loss: 0.3690\n","\n","Epoch 00265: val_loss did not improve from 0.36880\n","Epoch 266/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1349 - val_loss: 0.3690\n","\n","Epoch 00266: val_loss did not improve from 0.36880\n","Epoch 267/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1348 - val_loss: 0.3690\n","\n","Epoch 00267: val_loss did not improve from 0.36880\n","Epoch 268/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1347 - val_loss: 0.3690\n","\n","Epoch 00268: val_loss did not improve from 0.36880\n","Epoch 269/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1346 - val_loss: 0.3689\n","\n","Epoch 00269: val_loss did not improve from 0.36880\n","Epoch 270/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1345 - val_loss: 0.3689\n","\n","Epoch 00270: val_loss did not improve from 0.36880\n","Epoch 271/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1343 - val_loss: 0.3689\n","\n","Epoch 00271: val_loss did not improve from 0.36880\n","Epoch 272/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1342 - val_loss: 0.3690\n","\n","Epoch 00272: val_loss did not improve from 0.36880\n","Epoch 273/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1341 - val_loss: 0.3690\n","\n","Epoch 00273: val_loss did not improve from 0.36880\n","Epoch 274/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1340 - val_loss: 0.3690\n","\n","Epoch 00274: val_loss did not improve from 0.36880\n","Epoch 275/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1339 - val_loss: 0.3690\n","\n","Epoch 00275: val_loss did not improve from 0.36880\n","Epoch 276/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1338 - val_loss: 0.3690\n","\n","Epoch 00276: val_loss did not improve from 0.36880\n","Epoch 277/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1337 - val_loss: 0.3689\n","\n","Epoch 00277: val_loss did not improve from 0.36880\n","Epoch 278/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1336 - val_loss: 0.3689\n","\n","Epoch 00278: val_loss did not improve from 0.36880\n","Epoch 279/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1335 - val_loss: 0.3689\n","\n","Epoch 00279: val_loss did not improve from 0.36880\n","Epoch 280/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1333 - val_loss: 0.3689\n","\n","Epoch 00280: val_loss did not improve from 0.36880\n","Epoch 281/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1332 - val_loss: 0.3689\n","\n","Epoch 00281: val_loss did not improve from 0.36880\n","Epoch 282/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1331 - val_loss: 0.3689\n","\n","Epoch 00282: val_loss did not improve from 0.36880\n","Epoch 283/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1330 - val_loss: 0.3689\n","\n","Epoch 00283: val_loss did not improve from 0.36880\n","Epoch 284/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1329 - val_loss: 0.3689\n","\n","Epoch 00284: val_loss did not improve from 0.36880\n","Epoch 285/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1328 - val_loss: 0.3689\n","\n","Epoch 00285: val_loss did not improve from 0.36880\n","Epoch 286/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1327 - val_loss: 0.3689\n","\n","Epoch 00286: val_loss did not improve from 0.36880\n","Epoch 287/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1326 - val_loss: 0.3689\n","\n","Epoch 00287: val_loss did not improve from 0.36880\n","Epoch 288/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1325 - val_loss: 0.3689\n","\n","Epoch 00288: val_loss did not improve from 0.36880\n","Epoch 289/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1323 - val_loss: 0.3688\n","\n","Epoch 00289: val_loss did not improve from 0.36880\n","Epoch 290/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1322 - val_loss: 0.3688\n","\n","Epoch 00290: val_loss did not improve from 0.36880\n","Epoch 291/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1321 - val_loss: 0.3688\n","\n","Epoch 00291: val_loss did not improve from 0.36880\n","Epoch 292/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1320 - val_loss: 0.3688\n","\n","Epoch 00292: val_loss improved from 0.36880 to 0.36876, saving model to best_model.h5\n","Epoch 293/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1319 - val_loss: 0.3687\n","\n","Epoch 00293: val_loss improved from 0.36876 to 0.36873, saving model to best_model.h5\n","Epoch 294/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1318 - val_loss: 0.3687\n","\n","Epoch 00294: val_loss improved from 0.36873 to 0.36867, saving model to best_model.h5\n","Epoch 295/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1317 - val_loss: 0.3686\n","\n","Epoch 00295: val_loss improved from 0.36867 to 0.36859, saving model to best_model.h5\n","Epoch 296/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1316 - val_loss: 0.3685\n","\n","Epoch 00296: val_loss improved from 0.36859 to 0.36850, saving model to best_model.h5\n","Epoch 297/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1315 - val_loss: 0.3685\n","\n","Epoch 00297: val_loss improved from 0.36850 to 0.36845, saving model to best_model.h5\n","Epoch 298/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1314 - val_loss: 0.3684\n","\n","Epoch 00298: val_loss improved from 0.36845 to 0.36842, saving model to best_model.h5\n","Epoch 299/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1312 - val_loss: 0.3684\n","\n","Epoch 00299: val_loss improved from 0.36842 to 0.36838, saving model to best_model.h5\n","Epoch 300/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1311 - val_loss: 0.3683\n","\n","Epoch 00300: val_loss improved from 0.36838 to 0.36830, saving model to best_model.h5\n","Epoch 301/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1310 - val_loss: 0.3682\n","\n","Epoch 00301: val_loss improved from 0.36830 to 0.36823, saving model to best_model.h5\n","Epoch 302/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1309 - val_loss: 0.3682\n","\n","Epoch 00302: val_loss improved from 0.36823 to 0.36816, saving model to best_model.h5\n","Epoch 303/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1308 - val_loss: 0.3681\n","\n","Epoch 00303: val_loss improved from 0.36816 to 0.36807, saving model to best_model.h5\n","Epoch 304/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1307 - val_loss: 0.3680\n","\n","Epoch 00304: val_loss improved from 0.36807 to 0.36797, saving model to best_model.h5\n","Epoch 305/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1306 - val_loss: 0.3679\n","\n","Epoch 00305: val_loss improved from 0.36797 to 0.36785, saving model to best_model.h5\n","Epoch 306/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1305 - val_loss: 0.3678\n","\n","Epoch 00306: val_loss improved from 0.36785 to 0.36776, saving model to best_model.h5\n","Epoch 307/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1303 - val_loss: 0.3677\n","\n","Epoch 00307: val_loss improved from 0.36776 to 0.36769, saving model to best_model.h5\n","Epoch 308/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1302 - val_loss: 0.3676\n","\n","Epoch 00308: val_loss improved from 0.36769 to 0.36761, saving model to best_model.h5\n","Epoch 309/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1301 - val_loss: 0.3675\n","\n","Epoch 00309: val_loss improved from 0.36761 to 0.36755, saving model to best_model.h5\n","Epoch 310/2000\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1300 - val_loss: 0.3675\n","\n","Epoch 00310: val_loss improved from 0.36755 to 0.36747, saving model to best_model.h5\n","Epoch 311/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1299 - val_loss: 0.3674\n","\n","Epoch 00311: val_loss improved from 0.36747 to 0.36737, saving model to best_model.h5\n","Epoch 312/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1298 - val_loss: 0.3672\n","\n","Epoch 00312: val_loss improved from 0.36737 to 0.36724, saving model to best_model.h5\n","Epoch 313/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1297 - val_loss: 0.3671\n","\n","Epoch 00313: val_loss improved from 0.36724 to 0.36711, saving model to best_model.h5\n","Epoch 314/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1295 - val_loss: 0.3670\n","\n","Epoch 00314: val_loss improved from 0.36711 to 0.36698, saving model to best_model.h5\n","Epoch 315/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1294 - val_loss: 0.3669\n","\n","Epoch 00315: val_loss improved from 0.36698 to 0.36685, saving model to best_model.h5\n","Epoch 316/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1293 - val_loss: 0.3668\n","\n","Epoch 00316: val_loss improved from 0.36685 to 0.36679, saving model to best_model.h5\n","Epoch 317/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1292 - val_loss: 0.3668\n","\n","Epoch 00317: val_loss improved from 0.36679 to 0.36675, saving model to best_model.h5\n","Epoch 318/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1291 - val_loss: 0.3667\n","\n","Epoch 00318: val_loss improved from 0.36675 to 0.36672, saving model to best_model.h5\n","Epoch 319/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1290 - val_loss: 0.3667\n","\n","Epoch 00319: val_loss improved from 0.36672 to 0.36665, saving model to best_model.h5\n","Epoch 320/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1288 - val_loss: 0.3666\n","\n","Epoch 00320: val_loss improved from 0.36665 to 0.36656, saving model to best_model.h5\n","Epoch 321/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1287 - val_loss: 0.3664\n","\n","Epoch 00321: val_loss improved from 0.36656 to 0.36643, saving model to best_model.h5\n","Epoch 322/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1286 - val_loss: 0.3663\n","\n","Epoch 00322: val_loss improved from 0.36643 to 0.36631, saving model to best_model.h5\n","Epoch 323/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1285 - val_loss: 0.3662\n","\n","Epoch 00323: val_loss improved from 0.36631 to 0.36622, saving model to best_model.h5\n","Epoch 324/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1284 - val_loss: 0.3661\n","\n","Epoch 00324: val_loss improved from 0.36622 to 0.36611, saving model to best_model.h5\n","Epoch 325/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1283 - val_loss: 0.3660\n","\n","Epoch 00325: val_loss improved from 0.36611 to 0.36597, saving model to best_model.h5\n","Epoch 326/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1281 - val_loss: 0.3658\n","\n","Epoch 00326: val_loss improved from 0.36597 to 0.36584, saving model to best_model.h5\n","Epoch 327/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1280 - val_loss: 0.3657\n","\n","Epoch 00327: val_loss improved from 0.36584 to 0.36574, saving model to best_model.h5\n","Epoch 328/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1279 - val_loss: 0.3657\n","\n","Epoch 00328: val_loss improved from 0.36574 to 0.36567, saving model to best_model.h5\n","Epoch 329/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1278 - val_loss: 0.3656\n","\n","Epoch 00329: val_loss improved from 0.36567 to 0.36558, saving model to best_model.h5\n","Epoch 330/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1276 - val_loss: 0.3655\n","\n","Epoch 00330: val_loss improved from 0.36558 to 0.36548, saving model to best_model.h5\n","Epoch 331/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1275 - val_loss: 0.3654\n","\n","Epoch 00331: val_loss improved from 0.36548 to 0.36537, saving model to best_model.h5\n","Epoch 332/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1274 - val_loss: 0.3652\n","\n","Epoch 00332: val_loss improved from 0.36537 to 0.36521, saving model to best_model.h5\n","Epoch 333/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1273 - val_loss: 0.3650\n","\n","Epoch 00333: val_loss improved from 0.36521 to 0.36501, saving model to best_model.h5\n","Epoch 334/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1272 - val_loss: 0.3648\n","\n","Epoch 00334: val_loss improved from 0.36501 to 0.36484, saving model to best_model.h5\n","Epoch 335/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1270 - val_loss: 0.3647\n","\n","Epoch 00335: val_loss improved from 0.36484 to 0.36472, saving model to best_model.h5\n","Epoch 336/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1269 - val_loss: 0.3646\n","\n","Epoch 00336: val_loss improved from 0.36472 to 0.36462, saving model to best_model.h5\n","Epoch 337/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1268 - val_loss: 0.3645\n","\n","Epoch 00337: val_loss improved from 0.36462 to 0.36453, saving model to best_model.h5\n","Epoch 338/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1267 - val_loss: 0.3644\n","\n","Epoch 00338: val_loss improved from 0.36453 to 0.36442, saving model to best_model.h5\n","Epoch 339/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1265 - val_loss: 0.3643\n","\n","Epoch 00339: val_loss improved from 0.36442 to 0.36429, saving model to best_model.h5\n","Epoch 340/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1264 - val_loss: 0.3641\n","\n","Epoch 00340: val_loss improved from 0.36429 to 0.36412, saving model to best_model.h5\n","Epoch 341/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1263 - val_loss: 0.3640\n","\n","Epoch 00341: val_loss improved from 0.36412 to 0.36395, saving model to best_model.h5\n","Epoch 342/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1261 - val_loss: 0.3638\n","\n","Epoch 00342: val_loss improved from 0.36395 to 0.36380, saving model to best_model.h5\n","Epoch 343/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1260 - val_loss: 0.3637\n","\n","Epoch 00343: val_loss improved from 0.36380 to 0.36366, saving model to best_model.h5\n","Epoch 344/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1259 - val_loss: 0.3635\n","\n","Epoch 00344: val_loss improved from 0.36366 to 0.36351, saving model to best_model.h5\n","Epoch 345/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1258 - val_loss: 0.3634\n","\n","Epoch 00345: val_loss improved from 0.36351 to 0.36335, saving model to best_model.h5\n","Epoch 346/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1256 - val_loss: 0.3632\n","\n","Epoch 00346: val_loss improved from 0.36335 to 0.36323, saving model to best_model.h5\n","Epoch 347/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1255 - val_loss: 0.3631\n","\n","Epoch 00347: val_loss improved from 0.36323 to 0.36312, saving model to best_model.h5\n","Epoch 348/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1254 - val_loss: 0.3630\n","\n","Epoch 00348: val_loss improved from 0.36312 to 0.36301, saving model to best_model.h5\n","Epoch 349/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1252 - val_loss: 0.3629\n","\n","Epoch 00349: val_loss improved from 0.36301 to 0.36286, saving model to best_model.h5\n","Epoch 350/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1251 - val_loss: 0.3627\n","\n","Epoch 00350: val_loss improved from 0.36286 to 0.36269, saving model to best_model.h5\n","Epoch 351/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1250 - val_loss: 0.3625\n","\n","Epoch 00351: val_loss improved from 0.36269 to 0.36248, saving model to best_model.h5\n","Epoch 352/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1248 - val_loss: 0.3623\n","\n","Epoch 00352: val_loss improved from 0.36248 to 0.36227, saving model to best_model.h5\n","Epoch 353/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1247 - val_loss: 0.3621\n","\n","Epoch 00353: val_loss improved from 0.36227 to 0.36210, saving model to best_model.h5\n","Epoch 354/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1246 - val_loss: 0.3619\n","\n","Epoch 00354: val_loss improved from 0.36210 to 0.36194, saving model to best_model.h5\n","Epoch 355/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1244 - val_loss: 0.3618\n","\n","Epoch 00355: val_loss improved from 0.36194 to 0.36181, saving model to best_model.h5\n","Epoch 356/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1243 - val_loss: 0.3617\n","\n","Epoch 00356: val_loss improved from 0.36181 to 0.36166, saving model to best_model.h5\n","Epoch 357/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1242 - val_loss: 0.3615\n","\n","Epoch 00357: val_loss improved from 0.36166 to 0.36154, saving model to best_model.h5\n","Epoch 358/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1240 - val_loss: 0.3614\n","\n","Epoch 00358: val_loss improved from 0.36154 to 0.36139, saving model to best_model.h5\n","Epoch 359/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1239 - val_loss: 0.3612\n","\n","Epoch 00359: val_loss improved from 0.36139 to 0.36119, saving model to best_model.h5\n","Epoch 360/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1238 - val_loss: 0.3610\n","\n","Epoch 00360: val_loss improved from 0.36119 to 0.36100, saving model to best_model.h5\n","Epoch 361/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1236 - val_loss: 0.3608\n","\n","Epoch 00361: val_loss improved from 0.36100 to 0.36080, saving model to best_model.h5\n","Epoch 362/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1235 - val_loss: 0.3606\n","\n","Epoch 00362: val_loss improved from 0.36080 to 0.36064, saving model to best_model.h5\n","Epoch 363/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1234 - val_loss: 0.3605\n","\n","Epoch 00363: val_loss improved from 0.36064 to 0.36047, saving model to best_model.h5\n","Epoch 364/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1232 - val_loss: 0.3603\n","\n","Epoch 00364: val_loss improved from 0.36047 to 0.36030, saving model to best_model.h5\n","Epoch 365/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1231 - val_loss: 0.3601\n","\n","Epoch 00365: val_loss improved from 0.36030 to 0.36011, saving model to best_model.h5\n","Epoch 366/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1229 - val_loss: 0.3599\n","\n","Epoch 00366: val_loss improved from 0.36011 to 0.35992, saving model to best_model.h5\n","Epoch 367/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1228 - val_loss: 0.3597\n","\n","Epoch 00367: val_loss improved from 0.35992 to 0.35974, saving model to best_model.h5\n","Epoch 368/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1227 - val_loss: 0.3596\n","\n","Epoch 00368: val_loss improved from 0.35974 to 0.35960, saving model to best_model.h5\n","Epoch 369/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1225 - val_loss: 0.3594\n","\n","Epoch 00369: val_loss improved from 0.35960 to 0.35944, saving model to best_model.h5\n","Epoch 370/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1224 - val_loss: 0.3592\n","\n","Epoch 00370: val_loss improved from 0.35944 to 0.35924, saving model to best_model.h5\n","Epoch 371/2000\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1222 - val_loss: 0.3590\n","\n","Epoch 00371: val_loss improved from 0.35924 to 0.35902, saving model to best_model.h5\n","Epoch 372/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1221 - val_loss: 0.3588\n","\n","Epoch 00372: val_loss improved from 0.35902 to 0.35883, saving model to best_model.h5\n","Epoch 373/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1219 - val_loss: 0.3587\n","\n","Epoch 00373: val_loss improved from 0.35883 to 0.35866, saving model to best_model.h5\n","Epoch 374/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1218 - val_loss: 0.3585\n","\n","Epoch 00374: val_loss improved from 0.35866 to 0.35851, saving model to best_model.h5\n","Epoch 375/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1216 - val_loss: 0.3583\n","\n","Epoch 00375: val_loss improved from 0.35851 to 0.35835, saving model to best_model.h5\n","Epoch 376/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1215 - val_loss: 0.3582\n","\n","Epoch 00376: val_loss improved from 0.35835 to 0.35815, saving model to best_model.h5\n","Epoch 377/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1214 - val_loss: 0.3579\n","\n","Epoch 00377: val_loss improved from 0.35815 to 0.35794, saving model to best_model.h5\n","Epoch 378/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1212 - val_loss: 0.3577\n","\n","Epoch 00378: val_loss improved from 0.35794 to 0.35774, saving model to best_model.h5\n","Epoch 379/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1211 - val_loss: 0.3575\n","\n","Epoch 00379: val_loss improved from 0.35774 to 0.35752, saving model to best_model.h5\n","Epoch 380/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1209 - val_loss: 0.3573\n","\n","Epoch 00380: val_loss improved from 0.35752 to 0.35727, saving model to best_model.h5\n","Epoch 381/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1208 - val_loss: 0.3571\n","\n","Epoch 00381: val_loss improved from 0.35727 to 0.35706, saving model to best_model.h5\n","Epoch 382/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1206 - val_loss: 0.3569\n","\n","Epoch 00382: val_loss improved from 0.35706 to 0.35687, saving model to best_model.h5\n","Epoch 383/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1205 - val_loss: 0.3567\n","\n","Epoch 00383: val_loss improved from 0.35687 to 0.35671, saving model to best_model.h5\n","Epoch 384/2000\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1203 - val_loss: 0.3566\n","\n","Epoch 00384: val_loss improved from 0.35671 to 0.35656, saving model to best_model.h5\n","Epoch 385/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1202 - val_loss: 0.3564\n","\n","Epoch 00385: val_loss improved from 0.35656 to 0.35638, saving model to best_model.h5\n","Epoch 386/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1200 - val_loss: 0.3562\n","\n","Epoch 00386: val_loss improved from 0.35638 to 0.35616, saving model to best_model.h5\n","Epoch 387/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1199 - val_loss: 0.3560\n","\n","Epoch 00387: val_loss improved from 0.35616 to 0.35595, saving model to best_model.h5\n","Epoch 388/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1197 - val_loss: 0.3557\n","\n","Epoch 00388: val_loss improved from 0.35595 to 0.35574, saving model to best_model.h5\n","Epoch 389/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1196 - val_loss: 0.3555\n","\n","Epoch 00389: val_loss improved from 0.35574 to 0.35550, saving model to best_model.h5\n","Epoch 390/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1194 - val_loss: 0.3552\n","\n","Epoch 00390: val_loss improved from 0.35550 to 0.35525, saving model to best_model.h5\n","Epoch 391/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1193 - val_loss: 0.3550\n","\n","Epoch 00391: val_loss improved from 0.35525 to 0.35501, saving model to best_model.h5\n","Epoch 392/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1191 - val_loss: 0.3548\n","\n","Epoch 00392: val_loss improved from 0.35501 to 0.35480, saving model to best_model.h5\n","Epoch 393/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1190 - val_loss: 0.3546\n","\n","Epoch 00393: val_loss improved from 0.35480 to 0.35460, saving model to best_model.h5\n","Epoch 394/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1188 - val_loss: 0.3544\n","\n","Epoch 00394: val_loss improved from 0.35460 to 0.35438, saving model to best_model.h5\n","Epoch 395/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1187 - val_loss: 0.3542\n","\n","Epoch 00395: val_loss improved from 0.35438 to 0.35418, saving model to best_model.h5\n","Epoch 396/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1185 - val_loss: 0.3540\n","\n","Epoch 00396: val_loss improved from 0.35418 to 0.35399, saving model to best_model.h5\n","Epoch 397/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1183 - val_loss: 0.3538\n","\n","Epoch 00397: val_loss improved from 0.35399 to 0.35375, saving model to best_model.h5\n","Epoch 398/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1182 - val_loss: 0.3535\n","\n","Epoch 00398: val_loss improved from 0.35375 to 0.35352, saving model to best_model.h5\n","Epoch 399/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1180 - val_loss: 0.3533\n","\n","Epoch 00399: val_loss improved from 0.35352 to 0.35329, saving model to best_model.h5\n","Epoch 400/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1179 - val_loss: 0.3530\n","\n","Epoch 00400: val_loss improved from 0.35329 to 0.35304, saving model to best_model.h5\n","Epoch 401/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1177 - val_loss: 0.3528\n","\n","Epoch 00401: val_loss improved from 0.35304 to 0.35278, saving model to best_model.h5\n","Epoch 402/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1176 - val_loss: 0.3526\n","\n","Epoch 00402: val_loss improved from 0.35278 to 0.35259, saving model to best_model.h5\n","Epoch 403/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1174 - val_loss: 0.3524\n","\n","Epoch 00403: val_loss improved from 0.35259 to 0.35239, saving model to best_model.h5\n","Epoch 404/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1172 - val_loss: 0.3522\n","\n","Epoch 00404: val_loss improved from 0.35239 to 0.35217, saving model to best_model.h5\n","Epoch 405/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1171 - val_loss: 0.3519\n","\n","Epoch 00405: val_loss improved from 0.35217 to 0.35191, saving model to best_model.h5\n","Epoch 406/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1169 - val_loss: 0.3516\n","\n","Epoch 00406: val_loss improved from 0.35191 to 0.35164, saving model to best_model.h5\n","Epoch 407/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1168 - val_loss: 0.3514\n","\n","Epoch 00407: val_loss improved from 0.35164 to 0.35140, saving model to best_model.h5\n","Epoch 408/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1166 - val_loss: 0.3511\n","\n","Epoch 00408: val_loss improved from 0.35140 to 0.35113, saving model to best_model.h5\n","Epoch 409/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1164 - val_loss: 0.3508\n","\n","Epoch 00409: val_loss improved from 0.35113 to 0.35079, saving model to best_model.h5\n","Epoch 410/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1163 - val_loss: 0.3505\n","\n","Epoch 00410: val_loss improved from 0.35079 to 0.35050, saving model to best_model.h5\n","Epoch 411/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1161 - val_loss: 0.3502\n","\n","Epoch 00411: val_loss improved from 0.35050 to 0.35022, saving model to best_model.h5\n","Epoch 412/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1159 - val_loss: 0.3500\n","\n","Epoch 00412: val_loss improved from 0.35022 to 0.34997, saving model to best_model.h5\n","Epoch 413/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1158 - val_loss: 0.3497\n","\n","Epoch 00413: val_loss improved from 0.34997 to 0.34972, saving model to best_model.h5\n","Epoch 414/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1156 - val_loss: 0.3495\n","\n","Epoch 00414: val_loss improved from 0.34972 to 0.34949, saving model to best_model.h5\n","Epoch 415/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1154 - val_loss: 0.3492\n","\n","Epoch 00415: val_loss improved from 0.34949 to 0.34919, saving model to best_model.h5\n","Epoch 416/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1153 - val_loss: 0.3489\n","\n","Epoch 00416: val_loss improved from 0.34919 to 0.34887, saving model to best_model.h5\n","Epoch 417/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1151 - val_loss: 0.3485\n","\n","Epoch 00417: val_loss improved from 0.34887 to 0.34851, saving model to best_model.h5\n","Epoch 418/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1149 - val_loss: 0.3482\n","\n","Epoch 00418: val_loss improved from 0.34851 to 0.34820, saving model to best_model.h5\n","Epoch 419/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1147 - val_loss: 0.3480\n","\n","Epoch 00419: val_loss improved from 0.34820 to 0.34796, saving model to best_model.h5\n","Epoch 420/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1146 - val_loss: 0.3477\n","\n","Epoch 00420: val_loss improved from 0.34796 to 0.34774, saving model to best_model.h5\n","Epoch 421/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1144 - val_loss: 0.3475\n","\n","Epoch 00421: val_loss improved from 0.34774 to 0.34746, saving model to best_model.h5\n","Epoch 422/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1142 - val_loss: 0.3471\n","\n","Epoch 00422: val_loss improved from 0.34746 to 0.34714, saving model to best_model.h5\n","Epoch 423/2000\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1140 - val_loss: 0.3468\n","\n","Epoch 00423: val_loss improved from 0.34714 to 0.34682, saving model to best_model.h5\n","Epoch 424/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1139 - val_loss: 0.3465\n","\n","Epoch 00424: val_loss improved from 0.34682 to 0.34647, saving model to best_model.h5\n","Epoch 425/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1137 - val_loss: 0.3461\n","\n","Epoch 00425: val_loss improved from 0.34647 to 0.34610, saving model to best_model.h5\n","Epoch 426/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1135 - val_loss: 0.3458\n","\n","Epoch 00426: val_loss improved from 0.34610 to 0.34577, saving model to best_model.h5\n","Epoch 427/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1133 - val_loss: 0.3454\n","\n","Epoch 00427: val_loss improved from 0.34577 to 0.34544, saving model to best_model.h5\n","Epoch 428/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1131 - val_loss: 0.3451\n","\n","Epoch 00428: val_loss improved from 0.34544 to 0.34509, saving model to best_model.h5\n","Epoch 429/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1130 - val_loss: 0.3448\n","\n","Epoch 00429: val_loss improved from 0.34509 to 0.34477, saving model to best_model.h5\n","Epoch 430/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1128 - val_loss: 0.3444\n","\n","Epoch 00430: val_loss improved from 0.34477 to 0.34439, saving model to best_model.h5\n","Epoch 431/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1126 - val_loss: 0.3440\n","\n","Epoch 00431: val_loss improved from 0.34439 to 0.34405, saving model to best_model.h5\n","Epoch 432/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1124 - val_loss: 0.3437\n","\n","Epoch 00432: val_loss improved from 0.34405 to 0.34373, saving model to best_model.h5\n","Epoch 433/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1122 - val_loss: 0.3434\n","\n","Epoch 00433: val_loss improved from 0.34373 to 0.34340, saving model to best_model.h5\n","Epoch 434/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1120 - val_loss: 0.3431\n","\n","Epoch 00434: val_loss improved from 0.34340 to 0.34309, saving model to best_model.h5\n","Epoch 435/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1119 - val_loss: 0.3427\n","\n","Epoch 00435: val_loss improved from 0.34309 to 0.34273, saving model to best_model.h5\n","Epoch 436/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1117 - val_loss: 0.3424\n","\n","Epoch 00436: val_loss improved from 0.34273 to 0.34235, saving model to best_model.h5\n","Epoch 437/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1115 - val_loss: 0.3420\n","\n","Epoch 00437: val_loss improved from 0.34235 to 0.34197, saving model to best_model.h5\n","Epoch 438/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1113 - val_loss: 0.3416\n","\n","Epoch 00438: val_loss improved from 0.34197 to 0.34157, saving model to best_model.h5\n","Epoch 439/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1111 - val_loss: 0.3412\n","\n","Epoch 00439: val_loss improved from 0.34157 to 0.34119, saving model to best_model.h5\n","Epoch 440/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1109 - val_loss: 0.3409\n","\n","Epoch 00440: val_loss improved from 0.34119 to 0.34087, saving model to best_model.h5\n","Epoch 441/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1107 - val_loss: 0.3405\n","\n","Epoch 00441: val_loss improved from 0.34087 to 0.34052, saving model to best_model.h5\n","Epoch 442/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1105 - val_loss: 0.3402\n","\n","Epoch 00442: val_loss improved from 0.34052 to 0.34016, saving model to best_model.h5\n","Epoch 443/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1103 - val_loss: 0.3398\n","\n","Epoch 00443: val_loss improved from 0.34016 to 0.33979, saving model to best_model.h5\n","Epoch 444/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1101 - val_loss: 0.3394\n","\n","Epoch 00444: val_loss improved from 0.33979 to 0.33936, saving model to best_model.h5\n","Epoch 445/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1099 - val_loss: 0.3389\n","\n","Epoch 00445: val_loss improved from 0.33936 to 0.33893, saving model to best_model.h5\n","Epoch 446/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1097 - val_loss: 0.3386\n","\n","Epoch 00446: val_loss improved from 0.33893 to 0.33858, saving model to best_model.h5\n","Epoch 447/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1095 - val_loss: 0.3383\n","\n","Epoch 00447: val_loss improved from 0.33858 to 0.33828, saving model to best_model.h5\n","Epoch 448/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1093 - val_loss: 0.3380\n","\n","Epoch 00448: val_loss improved from 0.33828 to 0.33799, saving model to best_model.h5\n","Epoch 449/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1091 - val_loss: 0.3377\n","\n","Epoch 00449: val_loss improved from 0.33799 to 0.33767, saving model to best_model.h5\n","Epoch 450/2000\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1089 - val_loss: 0.3373\n","\n","Epoch 00450: val_loss improved from 0.33767 to 0.33727, saving model to best_model.h5\n","Epoch 451/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1087 - val_loss: 0.3368\n","\n","Epoch 00451: val_loss improved from 0.33727 to 0.33683, saving model to best_model.h5\n","Epoch 452/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1085 - val_loss: 0.3364\n","\n","Epoch 00452: val_loss improved from 0.33683 to 0.33638, saving model to best_model.h5\n","Epoch 453/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1083 - val_loss: 0.3360\n","\n","Epoch 00453: val_loss improved from 0.33638 to 0.33600, saving model to best_model.h5\n","Epoch 454/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1081 - val_loss: 0.3357\n","\n","Epoch 00454: val_loss improved from 0.33600 to 0.33567, saving model to best_model.h5\n","Epoch 455/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1079 - val_loss: 0.3354\n","\n","Epoch 00455: val_loss improved from 0.33567 to 0.33538, saving model to best_model.h5\n","Epoch 456/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1077 - val_loss: 0.3350\n","\n","Epoch 00456: val_loss improved from 0.33538 to 0.33504, saving model to best_model.h5\n","Epoch 457/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1075 - val_loss: 0.3347\n","\n","Epoch 00457: val_loss improved from 0.33504 to 0.33468, saving model to best_model.h5\n","Epoch 458/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1073 - val_loss: 0.3343\n","\n","Epoch 00458: val_loss improved from 0.33468 to 0.33428, saving model to best_model.h5\n","Epoch 459/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1071 - val_loss: 0.3339\n","\n","Epoch 00459: val_loss improved from 0.33428 to 0.33387, saving model to best_model.h5\n","Epoch 460/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1069 - val_loss: 0.3335\n","\n","Epoch 00460: val_loss improved from 0.33387 to 0.33352, saving model to best_model.h5\n","Epoch 461/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1067 - val_loss: 0.3332\n","\n","Epoch 00461: val_loss improved from 0.33352 to 0.33320, saving model to best_model.h5\n","Epoch 462/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1065 - val_loss: 0.3329\n","\n","Epoch 00462: val_loss improved from 0.33320 to 0.33290, saving model to best_model.h5\n","Epoch 463/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1062 - val_loss: 0.3326\n","\n","Epoch 00463: val_loss improved from 0.33290 to 0.33261, saving model to best_model.h5\n","Epoch 464/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1060 - val_loss: 0.3323\n","\n","Epoch 00464: val_loss improved from 0.33261 to 0.33230, saving model to best_model.h5\n","Epoch 465/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1058 - val_loss: 0.3319\n","\n","Epoch 00465: val_loss improved from 0.33230 to 0.33195, saving model to best_model.h5\n","Epoch 466/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1056 - val_loss: 0.3315\n","\n","Epoch 00466: val_loss improved from 0.33195 to 0.33153, saving model to best_model.h5\n","Epoch 467/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1054 - val_loss: 0.3311\n","\n","Epoch 00467: val_loss improved from 0.33153 to 0.33113, saving model to best_model.h5\n","Epoch 468/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1052 - val_loss: 0.3308\n","\n","Epoch 00468: val_loss improved from 0.33113 to 0.33079, saving model to best_model.h5\n","Epoch 469/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.1050 - val_loss: 0.3305\n","\n","Epoch 00469: val_loss improved from 0.33079 to 0.33048, saving model to best_model.h5\n","Epoch 470/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1047 - val_loss: 0.3302\n","\n","Epoch 00470: val_loss improved from 0.33048 to 0.33016, saving model to best_model.h5\n","Epoch 471/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1045 - val_loss: 0.3298\n","\n","Epoch 00471: val_loss improved from 0.33016 to 0.32983, saving model to best_model.h5\n","Epoch 472/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1043 - val_loss: 0.3295\n","\n","Epoch 00472: val_loss improved from 0.32983 to 0.32953, saving model to best_model.h5\n","Epoch 473/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1041 - val_loss: 0.3292\n","\n","Epoch 00473: val_loss improved from 0.32953 to 0.32918, saving model to best_model.h5\n","Epoch 474/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1039 - val_loss: 0.3288\n","\n","Epoch 00474: val_loss improved from 0.32918 to 0.32877, saving model to best_model.h5\n","Epoch 475/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1037 - val_loss: 0.3284\n","\n","Epoch 00475: val_loss improved from 0.32877 to 0.32836, saving model to best_model.h5\n","Epoch 476/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1034 - val_loss: 0.3280\n","\n","Epoch 00476: val_loss improved from 0.32836 to 0.32798, saving model to best_model.h5\n","Epoch 477/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1032 - val_loss: 0.3277\n","\n","Epoch 00477: val_loss improved from 0.32798 to 0.32766, saving model to best_model.h5\n","Epoch 478/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1030 - val_loss: 0.3274\n","\n","Epoch 00478: val_loss improved from 0.32766 to 0.32739, saving model to best_model.h5\n","Epoch 479/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.1028 - val_loss: 0.3271\n","\n","Epoch 00479: val_loss improved from 0.32739 to 0.32707, saving model to best_model.h5\n","Epoch 480/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1026 - val_loss: 0.3267\n","\n","Epoch 00480: val_loss improved from 0.32707 to 0.32674, saving model to best_model.h5\n","Epoch 481/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1024 - val_loss: 0.3264\n","\n","Epoch 00481: val_loss improved from 0.32674 to 0.32637, saving model to best_model.h5\n","Epoch 482/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1021 - val_loss: 0.3260\n","\n","Epoch 00482: val_loss improved from 0.32637 to 0.32600, saving model to best_model.h5\n","Epoch 483/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1019 - val_loss: 0.3256\n","\n","Epoch 00483: val_loss improved from 0.32600 to 0.32563, saving model to best_model.h5\n","Epoch 484/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1017 - val_loss: 0.3253\n","\n","Epoch 00484: val_loss improved from 0.32563 to 0.32526, saving model to best_model.h5\n","Epoch 485/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1015 - val_loss: 0.3249\n","\n","Epoch 00485: val_loss improved from 0.32526 to 0.32485, saving model to best_model.h5\n","Epoch 486/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1013 - val_loss: 0.3244\n","\n","Epoch 00486: val_loss improved from 0.32485 to 0.32441, saving model to best_model.h5\n","Epoch 487/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1010 - val_loss: 0.3240\n","\n","Epoch 00487: val_loss improved from 0.32441 to 0.32402, saving model to best_model.h5\n","Epoch 488/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1008 - val_loss: 0.3236\n","\n","Epoch 00488: val_loss improved from 0.32402 to 0.32361, saving model to best_model.h5\n","Epoch 489/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1006 - val_loss: 0.3232\n","\n","Epoch 00489: val_loss improved from 0.32361 to 0.32324, saving model to best_model.h5\n","Epoch 490/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1004 - val_loss: 0.3229\n","\n","Epoch 00490: val_loss improved from 0.32324 to 0.32293, saving model to best_model.h5\n","Epoch 491/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1001 - val_loss: 0.3225\n","\n","Epoch 00491: val_loss improved from 0.32293 to 0.32254, saving model to best_model.h5\n","Epoch 492/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0999 - val_loss: 0.3221\n","\n","Epoch 00492: val_loss improved from 0.32254 to 0.32207, saving model to best_model.h5\n","Epoch 493/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0997 - val_loss: 0.3216\n","\n","Epoch 00493: val_loss improved from 0.32207 to 0.32157, saving model to best_model.h5\n","Epoch 494/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0994 - val_loss: 0.3211\n","\n","Epoch 00494: val_loss improved from 0.32157 to 0.32106, saving model to best_model.h5\n","Epoch 495/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0992 - val_loss: 0.3206\n","\n","Epoch 00495: val_loss improved from 0.32106 to 0.32061, saving model to best_model.h5\n","Epoch 496/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0990 - val_loss: 0.3202\n","\n","Epoch 00496: val_loss improved from 0.32061 to 0.32024, saving model to best_model.h5\n","Epoch 497/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0988 - val_loss: 0.3199\n","\n","Epoch 00497: val_loss improved from 0.32024 to 0.31989, saving model to best_model.h5\n","Epoch 498/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0985 - val_loss: 0.3196\n","\n","Epoch 00498: val_loss improved from 0.31989 to 0.31955, saving model to best_model.h5\n","Epoch 499/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0983 - val_loss: 0.3191\n","\n","Epoch 00499: val_loss improved from 0.31955 to 0.31913, saving model to best_model.h5\n","Epoch 500/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0981 - val_loss: 0.3187\n","\n","Epoch 00500: val_loss improved from 0.31913 to 0.31870, saving model to best_model.h5\n","Epoch 501/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0979 - val_loss: 0.3183\n","\n","Epoch 00501: val_loss improved from 0.31870 to 0.31830, saving model to best_model.h5\n","Epoch 502/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0976 - val_loss: 0.3179\n","\n","Epoch 00502: val_loss improved from 0.31830 to 0.31795, saving model to best_model.h5\n","Epoch 503/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0974 - val_loss: 0.3176\n","\n","Epoch 00503: val_loss improved from 0.31795 to 0.31760, saving model to best_model.h5\n","Epoch 504/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0972 - val_loss: 0.3172\n","\n","Epoch 00504: val_loss improved from 0.31760 to 0.31718, saving model to best_model.h5\n","Epoch 505/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0969 - val_loss: 0.3167\n","\n","Epoch 00505: val_loss improved from 0.31718 to 0.31673, saving model to best_model.h5\n","Epoch 506/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0967 - val_loss: 0.3163\n","\n","Epoch 00506: val_loss improved from 0.31673 to 0.31629, saving model to best_model.h5\n","Epoch 507/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0965 - val_loss: 0.3158\n","\n","Epoch 00507: val_loss improved from 0.31629 to 0.31584, saving model to best_model.h5\n","Epoch 508/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0963 - val_loss: 0.3154\n","\n","Epoch 00508: val_loss improved from 0.31584 to 0.31541, saving model to best_model.h5\n","Epoch 509/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0960 - val_loss: 0.3150\n","\n","Epoch 00509: val_loss improved from 0.31541 to 0.31502, saving model to best_model.h5\n","Epoch 510/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0958 - val_loss: 0.3145\n","\n","Epoch 00510: val_loss improved from 0.31502 to 0.31454, saving model to best_model.h5\n","Epoch 511/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0956 - val_loss: 0.3141\n","\n","Epoch 00511: val_loss improved from 0.31454 to 0.31406, saving model to best_model.h5\n","Epoch 512/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0953 - val_loss: 0.3136\n","\n","Epoch 00512: val_loss improved from 0.31406 to 0.31360, saving model to best_model.h5\n","Epoch 513/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0951 - val_loss: 0.3132\n","\n","Epoch 00513: val_loss improved from 0.31360 to 0.31316, saving model to best_model.h5\n","Epoch 514/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0949 - val_loss: 0.3127\n","\n","Epoch 00514: val_loss improved from 0.31316 to 0.31270, saving model to best_model.h5\n","Epoch 515/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0946 - val_loss: 0.3123\n","\n","Epoch 00515: val_loss improved from 0.31270 to 0.31227, saving model to best_model.h5\n","Epoch 516/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0944 - val_loss: 0.3118\n","\n","Epoch 00516: val_loss improved from 0.31227 to 0.31180, saving model to best_model.h5\n","Epoch 517/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0942 - val_loss: 0.3113\n","\n","Epoch 00517: val_loss improved from 0.31180 to 0.31129, saving model to best_model.h5\n","Epoch 518/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0939 - val_loss: 0.3108\n","\n","Epoch 00518: val_loss improved from 0.31129 to 0.31076, saving model to best_model.h5\n","Epoch 519/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0937 - val_loss: 0.3102\n","\n","Epoch 00519: val_loss improved from 0.31076 to 0.31023, saving model to best_model.h5\n","Epoch 520/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0935 - val_loss: 0.3097\n","\n","Epoch 00520: val_loss improved from 0.31023 to 0.30973, saving model to best_model.h5\n","Epoch 521/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0932 - val_loss: 0.3093\n","\n","Epoch 00521: val_loss improved from 0.30973 to 0.30929, saving model to best_model.h5\n","Epoch 522/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0930 - val_loss: 0.3089\n","\n","Epoch 00522: val_loss improved from 0.30929 to 0.30888, saving model to best_model.h5\n","Epoch 523/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0927 - val_loss: 0.3085\n","\n","Epoch 00523: val_loss improved from 0.30888 to 0.30850, saving model to best_model.h5\n","Epoch 524/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0925 - val_loss: 0.3081\n","\n","Epoch 00524: val_loss improved from 0.30850 to 0.30809, saving model to best_model.h5\n","Epoch 525/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0923 - val_loss: 0.3076\n","\n","Epoch 00525: val_loss improved from 0.30809 to 0.30762, saving model to best_model.h5\n","Epoch 526/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0920 - val_loss: 0.3071\n","\n","Epoch 00526: val_loss improved from 0.30762 to 0.30711, saving model to best_model.h5\n","Epoch 527/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0918 - val_loss: 0.3065\n","\n","Epoch 00527: val_loss improved from 0.30711 to 0.30651, saving model to best_model.h5\n","Epoch 528/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0916 - val_loss: 0.3060\n","\n","Epoch 00528: val_loss improved from 0.30651 to 0.30599, saving model to best_model.h5\n","Epoch 529/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0913 - val_loss: 0.3056\n","\n","Epoch 00529: val_loss improved from 0.30599 to 0.30555, saving model to best_model.h5\n","Epoch 530/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0911 - val_loss: 0.3052\n","\n","Epoch 00530: val_loss improved from 0.30555 to 0.30516, saving model to best_model.h5\n","Epoch 531/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0908 - val_loss: 0.3048\n","\n","Epoch 00531: val_loss improved from 0.30516 to 0.30477, saving model to best_model.h5\n","Epoch 532/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0906 - val_loss: 0.3043\n","\n","Epoch 00532: val_loss improved from 0.30477 to 0.30431, saving model to best_model.h5\n","Epoch 533/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0904 - val_loss: 0.3038\n","\n","Epoch 00533: val_loss improved from 0.30431 to 0.30383, saving model to best_model.h5\n","Epoch 534/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0901 - val_loss: 0.3034\n","\n","Epoch 00534: val_loss improved from 0.30383 to 0.30336, saving model to best_model.h5\n","Epoch 535/2000\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0899 - val_loss: 0.3029\n","\n","Epoch 00535: val_loss improved from 0.30336 to 0.30292, saving model to best_model.h5\n","Epoch 536/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0896 - val_loss: 0.3025\n","\n","Epoch 00536: val_loss improved from 0.30292 to 0.30248, saving model to best_model.h5\n","Epoch 537/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0894 - val_loss: 0.3020\n","\n","Epoch 00537: val_loss improved from 0.30248 to 0.30203, saving model to best_model.h5\n","Epoch 538/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0892 - val_loss: 0.3016\n","\n","Epoch 00538: val_loss improved from 0.30203 to 0.30161, saving model to best_model.h5\n","Epoch 539/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0889 - val_loss: 0.3012\n","\n","Epoch 00539: val_loss improved from 0.30161 to 0.30118, saving model to best_model.h5\n","Epoch 540/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0887 - val_loss: 0.3007\n","\n","Epoch 00540: val_loss improved from 0.30118 to 0.30073, saving model to best_model.h5\n","Epoch 541/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0884 - val_loss: 0.3003\n","\n","Epoch 00541: val_loss improved from 0.30073 to 0.30028, saving model to best_model.h5\n","Epoch 542/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0882 - val_loss: 0.2998\n","\n","Epoch 00542: val_loss improved from 0.30028 to 0.29976, saving model to best_model.h5\n","Epoch 543/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0879 - val_loss: 0.2993\n","\n","Epoch 00543: val_loss improved from 0.29976 to 0.29934, saving model to best_model.h5\n","Epoch 544/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0877 - val_loss: 0.2990\n","\n","Epoch 00544: val_loss improved from 0.29934 to 0.29898, saving model to best_model.h5\n","Epoch 545/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0875 - val_loss: 0.2986\n","\n","Epoch 00545: val_loss improved from 0.29898 to 0.29860, saving model to best_model.h5\n","Epoch 546/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0872 - val_loss: 0.2982\n","\n","Epoch 00546: val_loss improved from 0.29860 to 0.29816, saving model to best_model.h5\n","Epoch 547/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0869 - val_loss: 0.2977\n","\n","Epoch 00547: val_loss improved from 0.29816 to 0.29770, saving model to best_model.h5\n","Epoch 548/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0867 - val_loss: 0.2972\n","\n","Epoch 00548: val_loss improved from 0.29770 to 0.29718, saving model to best_model.h5\n","Epoch 549/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0865 - val_loss: 0.2966\n","\n","Epoch 00549: val_loss improved from 0.29718 to 0.29665, saving model to best_model.h5\n","Epoch 550/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0862 - val_loss: 0.2961\n","\n","Epoch 00550: val_loss improved from 0.29665 to 0.29614, saving model to best_model.h5\n","Epoch 551/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0860 - val_loss: 0.2957\n","\n","Epoch 00551: val_loss improved from 0.29614 to 0.29570, saving model to best_model.h5\n","Epoch 552/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0857 - val_loss: 0.2953\n","\n","Epoch 00552: val_loss improved from 0.29570 to 0.29531, saving model to best_model.h5\n","Epoch 553/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0854 - val_loss: 0.2949\n","\n","Epoch 00553: val_loss improved from 0.29531 to 0.29491, saving model to best_model.h5\n","Epoch 554/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0852 - val_loss: 0.2944\n","\n","Epoch 00554: val_loss improved from 0.29491 to 0.29442, saving model to best_model.h5\n","Epoch 555/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0850 - val_loss: 0.2939\n","\n","Epoch 00555: val_loss improved from 0.29442 to 0.29394, saving model to best_model.h5\n","Epoch 556/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0847 - val_loss: 0.2935\n","\n","Epoch 00556: val_loss improved from 0.29394 to 0.29351, saving model to best_model.h5\n","Epoch 557/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0844 - val_loss: 0.2931\n","\n","Epoch 00557: val_loss improved from 0.29351 to 0.29312, saving model to best_model.h5\n","Epoch 558/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0842 - val_loss: 0.2927\n","\n","Epoch 00558: val_loss improved from 0.29312 to 0.29273, saving model to best_model.h5\n","Epoch 559/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0840 - val_loss: 0.2923\n","\n","Epoch 00559: val_loss improved from 0.29273 to 0.29227, saving model to best_model.h5\n","Epoch 560/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0837 - val_loss: 0.2918\n","\n","Epoch 00560: val_loss improved from 0.29227 to 0.29180, saving model to best_model.h5\n","Epoch 561/2000\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0835 - val_loss: 0.2913\n","\n","Epoch 00561: val_loss improved from 0.29180 to 0.29130, saving model to best_model.h5\n","Epoch 562/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0832 - val_loss: 0.2908\n","\n","Epoch 00562: val_loss improved from 0.29130 to 0.29081, saving model to best_model.h5\n","Epoch 563/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0829 - val_loss: 0.2904\n","\n","Epoch 00563: val_loss improved from 0.29081 to 0.29040, saving model to best_model.h5\n","Epoch 564/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0827 - val_loss: 0.2900\n","\n","Epoch 00564: val_loss improved from 0.29040 to 0.29000, saving model to best_model.h5\n","Epoch 565/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0824 - val_loss: 0.2896\n","\n","Epoch 00565: val_loss improved from 0.29000 to 0.28962, saving model to best_model.h5\n","Epoch 566/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0822 - val_loss: 0.2893\n","\n","Epoch 00566: val_loss improved from 0.28962 to 0.28928, saving model to best_model.h5\n","Epoch 567/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0819 - val_loss: 0.2889\n","\n","Epoch 00567: val_loss improved from 0.28928 to 0.28889, saving model to best_model.h5\n","Epoch 568/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0817 - val_loss: 0.2885\n","\n","Epoch 00568: val_loss improved from 0.28889 to 0.28846, saving model to best_model.h5\n","Epoch 569/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0815 - val_loss: 0.2879\n","\n","Epoch 00569: val_loss improved from 0.28846 to 0.28793, saving model to best_model.h5\n","Epoch 570/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0812 - val_loss: 0.2874\n","\n","Epoch 00570: val_loss improved from 0.28793 to 0.28739, saving model to best_model.h5\n","Epoch 571/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0809 - val_loss: 0.2869\n","\n","Epoch 00571: val_loss improved from 0.28739 to 0.28692, saving model to best_model.h5\n","Epoch 572/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0807 - val_loss: 0.2865\n","\n","Epoch 00572: val_loss improved from 0.28692 to 0.28647, saving model to best_model.h5\n","Epoch 573/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0804 - val_loss: 0.2860\n","\n","Epoch 00573: val_loss improved from 0.28647 to 0.28601, saving model to best_model.h5\n","Epoch 574/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0802 - val_loss: 0.2856\n","\n","Epoch 00574: val_loss improved from 0.28601 to 0.28562, saving model to best_model.h5\n","Epoch 575/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0799 - val_loss: 0.2853\n","\n","Epoch 00575: val_loss improved from 0.28562 to 0.28526, saving model to best_model.h5\n","Epoch 576/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0797 - val_loss: 0.2849\n","\n","Epoch 00576: val_loss improved from 0.28526 to 0.28488, saving model to best_model.h5\n","Epoch 577/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0794 - val_loss: 0.2844\n","\n","Epoch 00577: val_loss improved from 0.28488 to 0.28443, saving model to best_model.h5\n","Epoch 578/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0792 - val_loss: 0.2839\n","\n","Epoch 00578: val_loss improved from 0.28443 to 0.28387, saving model to best_model.h5\n","Epoch 579/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0789 - val_loss: 0.2834\n","\n","Epoch 00579: val_loss improved from 0.28387 to 0.28337, saving model to best_model.h5\n","Epoch 580/2000\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0787 - val_loss: 0.2829\n","\n","Epoch 00580: val_loss improved from 0.28337 to 0.28294, saving model to best_model.h5\n","Epoch 581/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0784 - val_loss: 0.2826\n","\n","Epoch 00581: val_loss improved from 0.28294 to 0.28257, saving model to best_model.h5\n","Epoch 582/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0782 - val_loss: 0.2822\n","\n","Epoch 00582: val_loss improved from 0.28257 to 0.28220, saving model to best_model.h5\n","Epoch 583/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0779 - val_loss: 0.2818\n","\n","Epoch 00583: val_loss improved from 0.28220 to 0.28176, saving model to best_model.h5\n","Epoch 584/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0777 - val_loss: 0.2813\n","\n","Epoch 00584: val_loss improved from 0.28176 to 0.28126, saving model to best_model.h5\n","Epoch 585/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0774 - val_loss: 0.2807\n","\n","Epoch 00585: val_loss improved from 0.28126 to 0.28072, saving model to best_model.h5\n","Epoch 586/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0771 - val_loss: 0.2802\n","\n","Epoch 00586: val_loss improved from 0.28072 to 0.28023, saving model to best_model.h5\n","Epoch 587/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0769 - val_loss: 0.2798\n","\n","Epoch 00587: val_loss improved from 0.28023 to 0.27984, saving model to best_model.h5\n","Epoch 588/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0766 - val_loss: 0.2795\n","\n","Epoch 00588: val_loss improved from 0.27984 to 0.27947, saving model to best_model.h5\n","Epoch 589/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0764 - val_loss: 0.2791\n","\n","Epoch 00589: val_loss improved from 0.27947 to 0.27907, saving model to best_model.h5\n","Epoch 590/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0761 - val_loss: 0.2786\n","\n","Epoch 00590: val_loss improved from 0.27907 to 0.27861, saving model to best_model.h5\n","Epoch 591/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0759 - val_loss: 0.2782\n","\n","Epoch 00591: val_loss improved from 0.27861 to 0.27815, saving model to best_model.h5\n","Epoch 592/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0756 - val_loss: 0.2777\n","\n","Epoch 00592: val_loss improved from 0.27815 to 0.27774, saving model to best_model.h5\n","Epoch 593/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0754 - val_loss: 0.2774\n","\n","Epoch 00593: val_loss improved from 0.27774 to 0.27736, saving model to best_model.h5\n","Epoch 594/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0751 - val_loss: 0.2770\n","\n","Epoch 00594: val_loss improved from 0.27736 to 0.27698, saving model to best_model.h5\n","Epoch 595/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0749 - val_loss: 0.2766\n","\n","Epoch 00595: val_loss improved from 0.27698 to 0.27659, saving model to best_model.h5\n","Epoch 596/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0746 - val_loss: 0.2762\n","\n","Epoch 00596: val_loss improved from 0.27659 to 0.27619, saving model to best_model.h5\n","Epoch 597/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0744 - val_loss: 0.2758\n","\n","Epoch 00597: val_loss improved from 0.27619 to 0.27576, saving model to best_model.h5\n","Epoch 598/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0741 - val_loss: 0.2753\n","\n","Epoch 00598: val_loss improved from 0.27576 to 0.27532, saving model to best_model.h5\n","Epoch 599/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0739 - val_loss: 0.2749\n","\n","Epoch 00599: val_loss improved from 0.27532 to 0.27488, saving model to best_model.h5\n","Epoch 600/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0736 - val_loss: 0.2745\n","\n","Epoch 00600: val_loss improved from 0.27488 to 0.27447, saving model to best_model.h5\n","Epoch 601/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0734 - val_loss: 0.2741\n","\n","Epoch 00601: val_loss improved from 0.27447 to 0.27411, saving model to best_model.h5\n","Epoch 602/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0731 - val_loss: 0.2738\n","\n","Epoch 00602: val_loss improved from 0.27411 to 0.27377, saving model to best_model.h5\n","Epoch 603/2000\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0729 - val_loss: 0.2734\n","\n","Epoch 00603: val_loss improved from 0.27377 to 0.27336, saving model to best_model.h5\n","Epoch 604/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0726 - val_loss: 0.2729\n","\n","Epoch 00604: val_loss improved from 0.27336 to 0.27289, saving model to best_model.h5\n","Epoch 605/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0724 - val_loss: 0.2724\n","\n","Epoch 00605: val_loss improved from 0.27289 to 0.27239, saving model to best_model.h5\n","Epoch 606/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0721 - val_loss: 0.2720\n","\n","Epoch 00606: val_loss improved from 0.27239 to 0.27195, saving model to best_model.h5\n","Epoch 607/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0719 - val_loss: 0.2716\n","\n","Epoch 00607: val_loss improved from 0.27195 to 0.27157, saving model to best_model.h5\n","Epoch 608/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0716 - val_loss: 0.2712\n","\n","Epoch 00608: val_loss improved from 0.27157 to 0.27124, saving model to best_model.h5\n","Epoch 609/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0714 - val_loss: 0.2709\n","\n","Epoch 00609: val_loss improved from 0.27124 to 0.27086, saving model to best_model.h5\n","Epoch 610/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0711 - val_loss: 0.2704\n","\n","Epoch 00610: val_loss improved from 0.27086 to 0.27044, saving model to best_model.h5\n","Epoch 611/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0709 - val_loss: 0.2700\n","\n","Epoch 00611: val_loss improved from 0.27044 to 0.27001, saving model to best_model.h5\n","Epoch 612/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0706 - val_loss: 0.2696\n","\n","Epoch 00612: val_loss improved from 0.27001 to 0.26960, saving model to best_model.h5\n","Epoch 613/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0704 - val_loss: 0.2693\n","\n","Epoch 00613: val_loss improved from 0.26960 to 0.26925, saving model to best_model.h5\n","Epoch 614/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0701 - val_loss: 0.2689\n","\n","Epoch 00614: val_loss improved from 0.26925 to 0.26890, saving model to best_model.h5\n","Epoch 615/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0699 - val_loss: 0.2685\n","\n","Epoch 00615: val_loss improved from 0.26890 to 0.26853, saving model to best_model.h5\n","Epoch 616/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0696 - val_loss: 0.2682\n","\n","Epoch 00616: val_loss improved from 0.26853 to 0.26816, saving model to best_model.h5\n","Epoch 617/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0694 - val_loss: 0.2678\n","\n","Epoch 00617: val_loss improved from 0.26816 to 0.26775, saving model to best_model.h5\n","Epoch 618/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0691 - val_loss: 0.2673\n","\n","Epoch 00618: val_loss improved from 0.26775 to 0.26734, saving model to best_model.h5\n","Epoch 619/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0689 - val_loss: 0.2669\n","\n","Epoch 00619: val_loss improved from 0.26734 to 0.26693, saving model to best_model.h5\n","Epoch 620/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0686 - val_loss: 0.2666\n","\n","Epoch 00620: val_loss improved from 0.26693 to 0.26657, saving model to best_model.h5\n","Epoch 621/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0684 - val_loss: 0.2663\n","\n","Epoch 00621: val_loss improved from 0.26657 to 0.26625, saving model to best_model.h5\n","Epoch 622/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0681 - val_loss: 0.2660\n","\n","Epoch 00622: val_loss improved from 0.26625 to 0.26596, saving model to best_model.h5\n","Epoch 623/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0679 - val_loss: 0.2655\n","\n","Epoch 00623: val_loss improved from 0.26596 to 0.26555, saving model to best_model.h5\n","Epoch 624/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0676 - val_loss: 0.2651\n","\n","Epoch 00624: val_loss improved from 0.26555 to 0.26508, saving model to best_model.h5\n","Epoch 625/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0674 - val_loss: 0.2646\n","\n","Epoch 00625: val_loss improved from 0.26508 to 0.26463, saving model to best_model.h5\n","Epoch 626/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0672 - val_loss: 0.2643\n","\n","Epoch 00626: val_loss improved from 0.26463 to 0.26428, saving model to best_model.h5\n","Epoch 627/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0669 - val_loss: 0.2640\n","\n","Epoch 00627: val_loss improved from 0.26428 to 0.26401, saving model to best_model.h5\n","Epoch 628/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0667 - val_loss: 0.2637\n","\n","Epoch 00628: val_loss improved from 0.26401 to 0.26372, saving model to best_model.h5\n","Epoch 629/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0664 - val_loss: 0.2634\n","\n","Epoch 00629: val_loss improved from 0.26372 to 0.26340, saving model to best_model.h5\n","Epoch 630/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0662 - val_loss: 0.2630\n","\n","Epoch 00630: val_loss improved from 0.26340 to 0.26303, saving model to best_model.h5\n","Epoch 631/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0659 - val_loss: 0.2627\n","\n","Epoch 00631: val_loss improved from 0.26303 to 0.26267, saving model to best_model.h5\n","Epoch 632/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0657 - val_loss: 0.2623\n","\n","Epoch 00632: val_loss improved from 0.26267 to 0.26232, saving model to best_model.h5\n","Epoch 633/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0654 - val_loss: 0.2620\n","\n","Epoch 00633: val_loss improved from 0.26232 to 0.26200, saving model to best_model.h5\n","Epoch 634/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0652 - val_loss: 0.2617\n","\n","Epoch 00634: val_loss improved from 0.26200 to 0.26165, saving model to best_model.h5\n","Epoch 635/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0649 - val_loss: 0.2613\n","\n","Epoch 00635: val_loss improved from 0.26165 to 0.26129, saving model to best_model.h5\n","Epoch 636/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0647 - val_loss: 0.2610\n","\n","Epoch 00636: val_loss improved from 0.26129 to 0.26095, saving model to best_model.h5\n","Epoch 637/2000\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0645 - val_loss: 0.2606\n","\n","Epoch 00637: val_loss improved from 0.26095 to 0.26063, saving model to best_model.h5\n","Epoch 638/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0642 - val_loss: 0.2604\n","\n","Epoch 00638: val_loss improved from 0.26063 to 0.26037, saving model to best_model.h5\n","Epoch 639/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0640 - val_loss: 0.2601\n","\n","Epoch 00639: val_loss improved from 0.26037 to 0.26013, saving model to best_model.h5\n","Epoch 640/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0638 - val_loss: 0.2598\n","\n","Epoch 00640: val_loss improved from 0.26013 to 0.25981, saving model to best_model.h5\n","Epoch 641/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0635 - val_loss: 0.2595\n","\n","Epoch 00641: val_loss improved from 0.25981 to 0.25945, saving model to best_model.h5\n","Epoch 642/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0633 - val_loss: 0.2590\n","\n","Epoch 00642: val_loss improved from 0.25945 to 0.25904, saving model to best_model.h5\n","Epoch 643/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0630 - val_loss: 0.2586\n","\n","Epoch 00643: val_loss improved from 0.25904 to 0.25861, saving model to best_model.h5\n","Epoch 644/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0628 - val_loss: 0.2582\n","\n","Epoch 00644: val_loss improved from 0.25861 to 0.25821, saving model to best_model.h5\n","Epoch 645/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0626 - val_loss: 0.2579\n","\n","Epoch 00645: val_loss improved from 0.25821 to 0.25789, saving model to best_model.h5\n","Epoch 646/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0623 - val_loss: 0.2576\n","\n","Epoch 00646: val_loss improved from 0.25789 to 0.25762, saving model to best_model.h5\n","Epoch 647/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0621 - val_loss: 0.2574\n","\n","Epoch 00647: val_loss improved from 0.25762 to 0.25738, saving model to best_model.h5\n","Epoch 648/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0619 - val_loss: 0.2571\n","\n","Epoch 00648: val_loss improved from 0.25738 to 0.25712, saving model to best_model.h5\n","Epoch 649/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0616 - val_loss: 0.2568\n","\n","Epoch 00649: val_loss improved from 0.25712 to 0.25680, saving model to best_model.h5\n","Epoch 650/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0614 - val_loss: 0.2565\n","\n","Epoch 00650: val_loss improved from 0.25680 to 0.25648, saving model to best_model.h5\n","Epoch 651/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0611 - val_loss: 0.2562\n","\n","Epoch 00651: val_loss improved from 0.25648 to 0.25621, saving model to best_model.h5\n","Epoch 652/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0609 - val_loss: 0.2560\n","\n","Epoch 00652: val_loss improved from 0.25621 to 0.25596, saving model to best_model.h5\n","Epoch 653/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0607 - val_loss: 0.2556\n","\n","Epoch 00653: val_loss improved from 0.25596 to 0.25564, saving model to best_model.h5\n","Epoch 654/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0604 - val_loss: 0.2552\n","\n","Epoch 00654: val_loss improved from 0.25564 to 0.25523, saving model to best_model.h5\n","Epoch 655/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0602 - val_loss: 0.2548\n","\n","Epoch 00655: val_loss improved from 0.25523 to 0.25482, saving model to best_model.h5\n","Epoch 656/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0600 - val_loss: 0.2545\n","\n","Epoch 00656: val_loss improved from 0.25482 to 0.25451, saving model to best_model.h5\n","Epoch 657/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0597 - val_loss: 0.2543\n","\n","Epoch 00657: val_loss improved from 0.25451 to 0.25427, saving model to best_model.h5\n","Epoch 658/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0595 - val_loss: 0.2541\n","\n","Epoch 00658: val_loss improved from 0.25427 to 0.25407, saving model to best_model.h5\n","Epoch 659/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0593 - val_loss: 0.2539\n","\n","Epoch 00659: val_loss improved from 0.25407 to 0.25387, saving model to best_model.h5\n","Epoch 660/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0590 - val_loss: 0.2536\n","\n","Epoch 00660: val_loss improved from 0.25387 to 0.25363, saving model to best_model.h5\n","Epoch 661/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0588 - val_loss: 0.2533\n","\n","Epoch 00661: val_loss improved from 0.25363 to 0.25328, saving model to best_model.h5\n","Epoch 662/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0586 - val_loss: 0.2529\n","\n","Epoch 00662: val_loss improved from 0.25328 to 0.25293, saving model to best_model.h5\n","Epoch 663/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0583 - val_loss: 0.2526\n","\n","Epoch 00663: val_loss improved from 0.25293 to 0.25258, saving model to best_model.h5\n","Epoch 664/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0581 - val_loss: 0.2523\n","\n","Epoch 00664: val_loss improved from 0.25258 to 0.25230, saving model to best_model.h5\n","Epoch 665/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0578 - val_loss: 0.2521\n","\n","Epoch 00665: val_loss improved from 0.25230 to 0.25209, saving model to best_model.h5\n","Epoch 666/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0576 - val_loss: 0.2519\n","\n","Epoch 00666: val_loss improved from 0.25209 to 0.25186, saving model to best_model.h5\n","Epoch 667/2000\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0574 - val_loss: 0.2516\n","\n","Epoch 00667: val_loss improved from 0.25186 to 0.25158, saving model to best_model.h5\n","Epoch 668/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0572 - val_loss: 0.2513\n","\n","Epoch 00668: val_loss improved from 0.25158 to 0.25131, saving model to best_model.h5\n","Epoch 669/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0569 - val_loss: 0.2510\n","\n","Epoch 00669: val_loss improved from 0.25131 to 0.25105, saving model to best_model.h5\n","Epoch 670/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0567 - val_loss: 0.2508\n","\n","Epoch 00670: val_loss improved from 0.25105 to 0.25081, saving model to best_model.h5\n","Epoch 671/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0565 - val_loss: 0.2506\n","\n","Epoch 00671: val_loss improved from 0.25081 to 0.25058, saving model to best_model.h5\n","Epoch 672/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0563 - val_loss: 0.2503\n","\n","Epoch 00672: val_loss improved from 0.25058 to 0.25033, saving model to best_model.h5\n","Epoch 673/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0560 - val_loss: 0.2500\n","\n","Epoch 00673: val_loss improved from 0.25033 to 0.25003, saving model to best_model.h5\n","Epoch 674/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0558 - val_loss: 0.2497\n","\n","Epoch 00674: val_loss improved from 0.25003 to 0.24972, saving model to best_model.h5\n","Epoch 675/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0556 - val_loss: 0.2494\n","\n","Epoch 00675: val_loss improved from 0.24972 to 0.24939, saving model to best_model.h5\n","Epoch 676/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0554 - val_loss: 0.2491\n","\n","Epoch 00676: val_loss improved from 0.24939 to 0.24906, saving model to best_model.h5\n","Epoch 677/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0551 - val_loss: 0.2488\n","\n","Epoch 00677: val_loss improved from 0.24906 to 0.24878, saving model to best_model.h5\n","Epoch 678/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0549 - val_loss: 0.2485\n","\n","Epoch 00678: val_loss improved from 0.24878 to 0.24853, saving model to best_model.h5\n","Epoch 679/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0547 - val_loss: 0.2483\n","\n","Epoch 00679: val_loss improved from 0.24853 to 0.24827, saving model to best_model.h5\n","Epoch 680/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0545 - val_loss: 0.2480\n","\n","Epoch 00680: val_loss improved from 0.24827 to 0.24796, saving model to best_model.h5\n","Epoch 681/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0542 - val_loss: 0.2477\n","\n","Epoch 00681: val_loss improved from 0.24796 to 0.24766, saving model to best_model.h5\n","Epoch 682/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0540 - val_loss: 0.2474\n","\n","Epoch 00682: val_loss improved from 0.24766 to 0.24741, saving model to best_model.h5\n","Epoch 683/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0538 - val_loss: 0.2472\n","\n","Epoch 00683: val_loss improved from 0.24741 to 0.24719, saving model to best_model.h5\n","Epoch 684/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0536 - val_loss: 0.2470\n","\n","Epoch 00684: val_loss improved from 0.24719 to 0.24699, saving model to best_model.h5\n","Epoch 685/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0534 - val_loss: 0.2467\n","\n","Epoch 00685: val_loss improved from 0.24699 to 0.24672, saving model to best_model.h5\n","Epoch 686/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0532 - val_loss: 0.2464\n","\n","Epoch 00686: val_loss improved from 0.24672 to 0.24639, saving model to best_model.h5\n","Epoch 687/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0529 - val_loss: 0.2460\n","\n","Epoch 00687: val_loss improved from 0.24639 to 0.24604, saving model to best_model.h5\n","Epoch 688/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0527 - val_loss: 0.2457\n","\n","Epoch 00688: val_loss improved from 0.24604 to 0.24571, saving model to best_model.h5\n","Epoch 689/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0525 - val_loss: 0.2455\n","\n","Epoch 00689: val_loss improved from 0.24571 to 0.24546, saving model to best_model.h5\n","Epoch 690/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0523 - val_loss: 0.2452\n","\n","Epoch 00690: val_loss improved from 0.24546 to 0.24523, saving model to best_model.h5\n","Epoch 691/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0521 - val_loss: 0.2450\n","\n","Epoch 00691: val_loss improved from 0.24523 to 0.24499, saving model to best_model.h5\n","Epoch 692/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0519 - val_loss: 0.2448\n","\n","Epoch 00692: val_loss improved from 0.24499 to 0.24475, saving model to best_model.h5\n","Epoch 693/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0517 - val_loss: 0.2445\n","\n","Epoch 00693: val_loss improved from 0.24475 to 0.24447, saving model to best_model.h5\n","Epoch 694/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0515 - val_loss: 0.2442\n","\n","Epoch 00694: val_loss improved from 0.24447 to 0.24417, saving model to best_model.h5\n","Epoch 695/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0513 - val_loss: 0.2439\n","\n","Epoch 00695: val_loss improved from 0.24417 to 0.24388, saving model to best_model.h5\n","Epoch 696/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0510 - val_loss: 0.2436\n","\n","Epoch 00696: val_loss improved from 0.24388 to 0.24358, saving model to best_model.h5\n","Epoch 697/2000\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0508 - val_loss: 0.2433\n","\n","Epoch 00697: val_loss improved from 0.24358 to 0.24330, saving model to best_model.h5\n","Epoch 698/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0506 - val_loss: 0.2430\n","\n","Epoch 00698: val_loss improved from 0.24330 to 0.24301, saving model to best_model.h5\n","Epoch 699/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0504 - val_loss: 0.2428\n","\n","Epoch 00699: val_loss improved from 0.24301 to 0.24276, saving model to best_model.h5\n","Epoch 700/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0502 - val_loss: 0.2426\n","\n","Epoch 00700: val_loss improved from 0.24276 to 0.24257, saving model to best_model.h5\n","Epoch 701/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0500 - val_loss: 0.2424\n","\n","Epoch 00701: val_loss improved from 0.24257 to 0.24238, saving model to best_model.h5\n","Epoch 702/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0498 - val_loss: 0.2422\n","\n","Epoch 00702: val_loss improved from 0.24238 to 0.24217, saving model to best_model.h5\n","Epoch 703/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0496 - val_loss: 0.2419\n","\n","Epoch 00703: val_loss improved from 0.24217 to 0.24190, saving model to best_model.h5\n","Epoch 704/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0494 - val_loss: 0.2416\n","\n","Epoch 00704: val_loss improved from 0.24190 to 0.24161, saving model to best_model.h5\n","Epoch 705/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0492 - val_loss: 0.2413\n","\n","Epoch 00705: val_loss improved from 0.24161 to 0.24132, saving model to best_model.h5\n","Epoch 706/2000\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0490 - val_loss: 0.2410\n","\n","Epoch 00706: val_loss improved from 0.24132 to 0.24104, saving model to best_model.h5\n","Epoch 707/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0488 - val_loss: 0.2408\n","\n","Epoch 00707: val_loss improved from 0.24104 to 0.24080, saving model to best_model.h5\n","Epoch 708/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0486 - val_loss: 0.2406\n","\n","Epoch 00708: val_loss improved from 0.24080 to 0.24060, saving model to best_model.h5\n","Epoch 709/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0484 - val_loss: 0.2404\n","\n","Epoch 00709: val_loss improved from 0.24060 to 0.24044, saving model to best_model.h5\n","Epoch 710/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0482 - val_loss: 0.2403\n","\n","Epoch 00710: val_loss improved from 0.24044 to 0.24027, saving model to best_model.h5\n","Epoch 711/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0480 - val_loss: 0.2401\n","\n","Epoch 00711: val_loss improved from 0.24027 to 0.24006, saving model to best_model.h5\n","Epoch 712/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0478 - val_loss: 0.2398\n","\n","Epoch 00712: val_loss improved from 0.24006 to 0.23982, saving model to best_model.h5\n","Epoch 713/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0476 - val_loss: 0.2396\n","\n","Epoch 00713: val_loss improved from 0.23982 to 0.23956, saving model to best_model.h5\n","Epoch 714/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0474 - val_loss: 0.2393\n","\n","Epoch 00714: val_loss improved from 0.23956 to 0.23932, saving model to best_model.h5\n","Epoch 715/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0472 - val_loss: 0.2391\n","\n","Epoch 00715: val_loss improved from 0.23932 to 0.23912, saving model to best_model.h5\n","Epoch 716/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0470 - val_loss: 0.2390\n","\n","Epoch 00716: val_loss improved from 0.23912 to 0.23896, saving model to best_model.h5\n","Epoch 717/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0468 - val_loss: 0.2388\n","\n","Epoch 00717: val_loss improved from 0.23896 to 0.23880, saving model to best_model.h5\n","Epoch 718/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0466 - val_loss: 0.2386\n","\n","Epoch 00718: val_loss improved from 0.23880 to 0.23862, saving model to best_model.h5\n","Epoch 719/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0464 - val_loss: 0.2384\n","\n","Epoch 00719: val_loss improved from 0.23862 to 0.23841, saving model to best_model.h5\n","Epoch 720/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0462 - val_loss: 0.2382\n","\n","Epoch 00720: val_loss improved from 0.23841 to 0.23823, saving model to best_model.h5\n","Epoch 721/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0460 - val_loss: 0.2381\n","\n","Epoch 00721: val_loss improved from 0.23823 to 0.23808, saving model to best_model.h5\n","Epoch 722/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0458 - val_loss: 0.2379\n","\n","Epoch 00722: val_loss improved from 0.23808 to 0.23795, saving model to best_model.h5\n","Epoch 723/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0456 - val_loss: 0.2378\n","\n","Epoch 00723: val_loss improved from 0.23795 to 0.23776, saving model to best_model.h5\n","Epoch 724/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0454 - val_loss: 0.2375\n","\n","Epoch 00724: val_loss improved from 0.23776 to 0.23755, saving model to best_model.h5\n","Epoch 725/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0452 - val_loss: 0.2373\n","\n","Epoch 00725: val_loss improved from 0.23755 to 0.23734, saving model to best_model.h5\n","Epoch 726/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0450 - val_loss: 0.2372\n","\n","Epoch 00726: val_loss improved from 0.23734 to 0.23718, saving model to best_model.h5\n","Epoch 727/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0449 - val_loss: 0.2370\n","\n","Epoch 00727: val_loss improved from 0.23718 to 0.23705, saving model to best_model.h5\n","Epoch 728/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0447 - val_loss: 0.2369\n","\n","Epoch 00728: val_loss improved from 0.23705 to 0.23691, saving model to best_model.h5\n","Epoch 729/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0445 - val_loss: 0.2368\n","\n","Epoch 00729: val_loss improved from 0.23691 to 0.23678, saving model to best_model.h5\n","Epoch 730/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0443 - val_loss: 0.2366\n","\n","Epoch 00730: val_loss improved from 0.23678 to 0.23664, saving model to best_model.h5\n","Epoch 731/2000\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0441 - val_loss: 0.2365\n","\n","Epoch 00731: val_loss improved from 0.23664 to 0.23648, saving model to best_model.h5\n","Epoch 732/2000\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0439 - val_loss: 0.2363\n","\n","Epoch 00732: val_loss improved from 0.23648 to 0.23631, saving model to best_model.h5\n","Epoch 733/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0437 - val_loss: 0.2361\n","\n","Epoch 00733: val_loss improved from 0.23631 to 0.23611, saving model to best_model.h5\n","Epoch 734/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0435 - val_loss: 0.2359\n","\n","Epoch 00734: val_loss improved from 0.23611 to 0.23592, saving model to best_model.h5\n","Epoch 735/2000\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0433 - val_loss: 0.2357\n","\n","Epoch 00735: val_loss improved from 0.23592 to 0.23575, saving model to best_model.h5\n","Epoch 736/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0432 - val_loss: 0.2356\n","\n","Epoch 00736: val_loss improved from 0.23575 to 0.23560, saving model to best_model.h5\n","Epoch 737/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0430 - val_loss: 0.2355\n","\n","Epoch 00737: val_loss improved from 0.23560 to 0.23545, saving model to best_model.h5\n","Epoch 738/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0428 - val_loss: 0.2353\n","\n","Epoch 00738: val_loss improved from 0.23545 to 0.23532, saving model to best_model.h5\n","Epoch 739/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0426 - val_loss: 0.2352\n","\n","Epoch 00739: val_loss improved from 0.23532 to 0.23518, saving model to best_model.h5\n","Epoch 740/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0424 - val_loss: 0.2351\n","\n","Epoch 00740: val_loss improved from 0.23518 to 0.23505, saving model to best_model.h5\n","Epoch 741/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0422 - val_loss: 0.2349\n","\n","Epoch 00741: val_loss improved from 0.23505 to 0.23491, saving model to best_model.h5\n","Epoch 742/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0421 - val_loss: 0.2348\n","\n","Epoch 00742: val_loss improved from 0.23491 to 0.23475, saving model to best_model.h5\n","Epoch 743/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0419 - val_loss: 0.2346\n","\n","Epoch 00743: val_loss improved from 0.23475 to 0.23458, saving model to best_model.h5\n","Epoch 744/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0417 - val_loss: 0.2344\n","\n","Epoch 00744: val_loss improved from 0.23458 to 0.23442, saving model to best_model.h5\n","Epoch 745/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0415 - val_loss: 0.2343\n","\n","Epoch 00745: val_loss improved from 0.23442 to 0.23428, saving model to best_model.h5\n","Epoch 746/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0414 - val_loss: 0.2341\n","\n","Epoch 00746: val_loss improved from 0.23428 to 0.23414, saving model to best_model.h5\n","Epoch 747/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0412 - val_loss: 0.2340\n","\n","Epoch 00747: val_loss improved from 0.23414 to 0.23400, saving model to best_model.h5\n","Epoch 748/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0410 - val_loss: 0.2339\n","\n","Epoch 00748: val_loss improved from 0.23400 to 0.23386, saving model to best_model.h5\n","Epoch 749/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0408 - val_loss: 0.2337\n","\n","Epoch 00749: val_loss improved from 0.23386 to 0.23374, saving model to best_model.h5\n","Epoch 750/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0407 - val_loss: 0.2336\n","\n","Epoch 00750: val_loss improved from 0.23374 to 0.23362, saving model to best_model.h5\n","Epoch 751/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0405 - val_loss: 0.2335\n","\n","Epoch 00751: val_loss improved from 0.23362 to 0.23350, saving model to best_model.h5\n","Epoch 752/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0403 - val_loss: 0.2334\n","\n","Epoch 00752: val_loss improved from 0.23350 to 0.23336, saving model to best_model.h5\n","Epoch 753/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0401 - val_loss: 0.2332\n","\n","Epoch 00753: val_loss improved from 0.23336 to 0.23322, saving model to best_model.h5\n","Epoch 754/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0400 - val_loss: 0.2331\n","\n","Epoch 00754: val_loss improved from 0.23322 to 0.23308, saving model to best_model.h5\n","Epoch 755/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0398 - val_loss: 0.2330\n","\n","Epoch 00755: val_loss improved from 0.23308 to 0.23296, saving model to best_model.h5\n","Epoch 756/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0396 - val_loss: 0.2328\n","\n","Epoch 00756: val_loss improved from 0.23296 to 0.23284, saving model to best_model.h5\n","Epoch 757/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0394 - val_loss: 0.2327\n","\n","Epoch 00757: val_loss improved from 0.23284 to 0.23273, saving model to best_model.h5\n","Epoch 758/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0393 - val_loss: 0.2326\n","\n","Epoch 00758: val_loss improved from 0.23273 to 0.23263, saving model to best_model.h5\n","Epoch 759/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0391 - val_loss: 0.2325\n","\n","Epoch 00759: val_loss improved from 0.23263 to 0.23250, saving model to best_model.h5\n","Epoch 760/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0389 - val_loss: 0.2324\n","\n","Epoch 00760: val_loss improved from 0.23250 to 0.23238, saving model to best_model.h5\n","Epoch 761/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0388 - val_loss: 0.2323\n","\n","Epoch 00761: val_loss improved from 0.23238 to 0.23225, saving model to best_model.h5\n","Epoch 762/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0386 - val_loss: 0.2321\n","\n","Epoch 00762: val_loss improved from 0.23225 to 0.23213, saving model to best_model.h5\n","Epoch 763/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0384 - val_loss: 0.2320\n","\n","Epoch 00763: val_loss improved from 0.23213 to 0.23201, saving model to best_model.h5\n","Epoch 764/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0383 - val_loss: 0.2319\n","\n","Epoch 00764: val_loss improved from 0.23201 to 0.23190, saving model to best_model.h5\n","Epoch 765/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0381 - val_loss: 0.2318\n","\n","Epoch 00765: val_loss improved from 0.23190 to 0.23181, saving model to best_model.h5\n","Epoch 766/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0379 - val_loss: 0.2317\n","\n","Epoch 00766: val_loss improved from 0.23181 to 0.23174, saving model to best_model.h5\n","Epoch 767/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0378 - val_loss: 0.2317\n","\n","Epoch 00767: val_loss improved from 0.23174 to 0.23168, saving model to best_model.h5\n","Epoch 768/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0376 - val_loss: 0.2316\n","\n","Epoch 00768: val_loss improved from 0.23168 to 0.23159, saving model to best_model.h5\n","Epoch 769/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0374 - val_loss: 0.2315\n","\n","Epoch 00769: val_loss improved from 0.23159 to 0.23147, saving model to best_model.h5\n","Epoch 770/2000\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0373 - val_loss: 0.2313\n","\n","Epoch 00770: val_loss improved from 0.23147 to 0.23134, saving model to best_model.h5\n","Epoch 771/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0371 - val_loss: 0.2312\n","\n","Epoch 00771: val_loss improved from 0.23134 to 0.23123, saving model to best_model.h5\n","Epoch 772/2000\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0369 - val_loss: 0.2312\n","\n","Epoch 00772: val_loss improved from 0.23123 to 0.23116, saving model to best_model.h5\n","Epoch 773/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0368 - val_loss: 0.2311\n","\n","Epoch 00773: val_loss improved from 0.23116 to 0.23112, saving model to best_model.h5\n","Epoch 774/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0366 - val_loss: 0.2311\n","\n","Epoch 00774: val_loss improved from 0.23112 to 0.23108, saving model to best_model.h5\n","Epoch 775/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0365 - val_loss: 0.2310\n","\n","Epoch 00775: val_loss improved from 0.23108 to 0.23103, saving model to best_model.h5\n","Epoch 776/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0363 - val_loss: 0.2310\n","\n","Epoch 00776: val_loss improved from 0.23103 to 0.23097, saving model to best_model.h5\n","Epoch 777/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0361 - val_loss: 0.2309\n","\n","Epoch 00777: val_loss improved from 0.23097 to 0.23087, saving model to best_model.h5\n","Epoch 778/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0360 - val_loss: 0.2308\n","\n","Epoch 00778: val_loss improved from 0.23087 to 0.23075, saving model to best_model.h5\n","Epoch 779/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0358 - val_loss: 0.2307\n","\n","Epoch 00779: val_loss improved from 0.23075 to 0.23066, saving model to best_model.h5\n","Epoch 780/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0357 - val_loss: 0.2306\n","\n","Epoch 00780: val_loss improved from 0.23066 to 0.23061, saving model to best_model.h5\n","Epoch 781/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0355 - val_loss: 0.2306\n","\n","Epoch 00781: val_loss improved from 0.23061 to 0.23058, saving model to best_model.h5\n","Epoch 782/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0354 - val_loss: 0.2305\n","\n","Epoch 00782: val_loss improved from 0.23058 to 0.23053, saving model to best_model.h5\n","Epoch 783/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0352 - val_loss: 0.2305\n","\n","Epoch 00783: val_loss improved from 0.23053 to 0.23046, saving model to best_model.h5\n","Epoch 784/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0351 - val_loss: 0.2304\n","\n","Epoch 00784: val_loss improved from 0.23046 to 0.23039, saving model to best_model.h5\n","Epoch 785/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0349 - val_loss: 0.2303\n","\n","Epoch 00785: val_loss improved from 0.23039 to 0.23034, saving model to best_model.h5\n","Epoch 786/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0348 - val_loss: 0.2303\n","\n","Epoch 00786: val_loss improved from 0.23034 to 0.23029, saving model to best_model.h5\n","Epoch 787/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0346 - val_loss: 0.2302\n","\n","Epoch 00787: val_loss improved from 0.23029 to 0.23024, saving model to best_model.h5\n","Epoch 788/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0345 - val_loss: 0.2302\n","\n","Epoch 00788: val_loss improved from 0.23024 to 0.23018, saving model to best_model.h5\n","Epoch 789/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0343 - val_loss: 0.2301\n","\n","Epoch 00789: val_loss improved from 0.23018 to 0.23013, saving model to best_model.h5\n","Epoch 790/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0342 - val_loss: 0.2301\n","\n","Epoch 00790: val_loss improved from 0.23013 to 0.23011, saving model to best_model.h5\n","Epoch 791/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0340 - val_loss: 0.2301\n","\n","Epoch 00791: val_loss improved from 0.23011 to 0.23009, saving model to best_model.h5\n","Epoch 792/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0339 - val_loss: 0.2301\n","\n","Epoch 00792: val_loss improved from 0.23009 to 0.23005, saving model to best_model.h5\n","Epoch 793/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0337 - val_loss: 0.2300\n","\n","Epoch 00793: val_loss improved from 0.23005 to 0.23001, saving model to best_model.h5\n","Epoch 794/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0336 - val_loss: 0.2300\n","\n","Epoch 00794: val_loss improved from 0.23001 to 0.22998, saving model to best_model.h5\n","Epoch 795/2000\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0334 - val_loss: 0.2300\n","\n","Epoch 00795: val_loss improved from 0.22998 to 0.22995, saving model to best_model.h5\n","Epoch 796/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0333 - val_loss: 0.2299\n","\n","Epoch 00796: val_loss improved from 0.22995 to 0.22994, saving model to best_model.h5\n","Epoch 797/2000\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0331 - val_loss: 0.2299\n","\n","Epoch 00797: val_loss improved from 0.22994 to 0.22993, saving model to best_model.h5\n","Epoch 798/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0330 - val_loss: 0.2299\n","\n","Epoch 00798: val_loss did not improve from 0.22993\n","Epoch 799/2000\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0328 - val_loss: 0.2300\n","\n","Epoch 00799: val_loss did not improve from 0.22993\n","Epoch 800/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0327 - val_loss: 0.2299\n","\n","Epoch 00800: val_loss did not improve from 0.22993\n","Epoch 801/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0326 - val_loss: 0.2299\n","\n","Epoch 00801: val_loss improved from 0.22993 to 0.22992, saving model to best_model.h5\n","Epoch 802/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0324 - val_loss: 0.2299\n","\n","Epoch 00802: val_loss improved from 0.22992 to 0.22990, saving model to best_model.h5\n","Epoch 803/2000\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0323 - val_loss: 0.2299\n","\n","Epoch 00803: val_loss improved from 0.22990 to 0.22989, saving model to best_model.h5\n","Epoch 804/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0321 - val_loss: 0.2299\n","\n","Epoch 00804: val_loss did not improve from 0.22989\n","Epoch 805/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0320 - val_loss: 0.2299\n","\n","Epoch 00805: val_loss did not improve from 0.22989\n","Epoch 806/2000\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0318 - val_loss: 0.2299\n","\n","Epoch 00806: val_loss did not improve from 0.22989\n","Epoch 807/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0317 - val_loss: 0.2299\n","\n","Epoch 00807: val_loss improved from 0.22989 to 0.22989, saving model to best_model.h5\n","Epoch 808/2000\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0316 - val_loss: 0.2299\n","\n","Epoch 00808: val_loss improved from 0.22989 to 0.22988, saving model to best_model.h5\n","Epoch 809/2000\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0314 - val_loss: 0.2299\n","\n","Epoch 00809: val_loss did not improve from 0.22988\n","Epoch 810/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0313 - val_loss: 0.2299\n","\n","Epoch 00810: val_loss did not improve from 0.22988\n","Epoch 811/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0312 - val_loss: 0.2300\n","\n","Epoch 00811: val_loss did not improve from 0.22988\n","Epoch 812/2000\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0310 - val_loss: 0.2300\n","\n","Epoch 00812: val_loss did not improve from 0.22988\n","Epoch 813/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0309 - val_loss: 0.2300\n","\n","Epoch 00813: val_loss did not improve from 0.22988\n","Epoch 814/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0307 - val_loss: 0.2300\n","\n","Epoch 00814: val_loss did not improve from 0.22988\n","Epoch 815/2000\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0306 - val_loss: 0.2300\n","\n","Epoch 00815: val_loss did not improve from 0.22988\n","Epoch 816/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0305 - val_loss: 0.2300\n","\n","Epoch 00816: val_loss did not improve from 0.22988\n","Epoch 817/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0303 - val_loss: 0.2300\n","\n","Epoch 00817: val_loss did not improve from 0.22988\n","Epoch 818/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0302 - val_loss: 0.2300\n","\n","Epoch 00818: val_loss did not improve from 0.22988\n","Epoch 819/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0301 - val_loss: 0.2300\n","\n","Epoch 00819: val_loss did not improve from 0.22988\n","Epoch 820/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0299 - val_loss: 0.2300\n","\n","Epoch 00820: val_loss did not improve from 0.22988\n","Epoch 821/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0298 - val_loss: 0.2300\n","\n","Epoch 00821: val_loss did not improve from 0.22988\n","Epoch 822/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0297 - val_loss: 0.2301\n","\n","Epoch 00822: val_loss did not improve from 0.22988\n","Epoch 823/2000\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0296 - val_loss: 0.2301\n","\n","Epoch 00823: val_loss did not improve from 0.22988\n","Epoch 824/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0294 - val_loss: 0.2302\n","\n","Epoch 00824: val_loss did not improve from 0.22988\n","Epoch 825/2000\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0293 - val_loss: 0.2302\n","\n","Epoch 00825: val_loss did not improve from 0.22988\n","Epoch 826/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0292 - val_loss: 0.2303\n","\n","Epoch 00826: val_loss did not improve from 0.22988\n","Epoch 827/2000\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0290 - val_loss: 0.2303\n","\n","Epoch 00827: val_loss did not improve from 0.22988\n","Epoch 828/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0289 - val_loss: 0.2303\n","\n","Epoch 00828: val_loss did not improve from 0.22988\n","Epoch 829/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0288 - val_loss: 0.2303\n","\n","Epoch 00829: val_loss did not improve from 0.22988\n","Epoch 830/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0287 - val_loss: 0.2304\n","\n","Epoch 00830: val_loss did not improve from 0.22988\n","Epoch 831/2000\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0285 - val_loss: 0.2304\n","\n","Epoch 00831: val_loss did not improve from 0.22988\n","Epoch 832/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0284 - val_loss: 0.2305\n","\n","Epoch 00832: val_loss did not improve from 0.22988\n","Epoch 833/2000\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0283 - val_loss: 0.2306\n","\n","Epoch 00833: val_loss did not improve from 0.22988\n","Epoch 834/2000\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0282 - val_loss: 0.2306\n","\n","Epoch 00834: val_loss did not improve from 0.22988\n","Epoch 835/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0280 - val_loss: 0.2307\n","\n","Epoch 00835: val_loss did not improve from 0.22988\n","Epoch 836/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0279 - val_loss: 0.2307\n","\n","Epoch 00836: val_loss did not improve from 0.22988\n","Epoch 837/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0278 - val_loss: 0.2308\n","\n","Epoch 00837: val_loss did not improve from 0.22988\n","Epoch 838/2000\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0277 - val_loss: 0.2308\n","\n","Epoch 00838: val_loss did not improve from 0.22988\n","Epoch 839/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0276 - val_loss: 0.2308\n","\n","Epoch 00839: val_loss did not improve from 0.22988\n","Epoch 840/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0275 - val_loss: 0.2309\n","\n","Epoch 00840: val_loss did not improve from 0.22988\n","Epoch 841/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0273 - val_loss: 0.2309\n","\n","Epoch 00841: val_loss did not improve from 0.22988\n","Epoch 842/2000\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0272 - val_loss: 0.2310\n","\n","Epoch 00842: val_loss did not improve from 0.22988\n","Epoch 843/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0271 - val_loss: 0.2310\n","\n","Epoch 00843: val_loss did not improve from 0.22988\n","Epoch 844/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0270 - val_loss: 0.2310\n","\n","Epoch 00844: val_loss did not improve from 0.22988\n","Epoch 845/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0269 - val_loss: 0.2311\n","\n","Epoch 00845: val_loss did not improve from 0.22988\n","Epoch 846/2000\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0268 - val_loss: 0.2311\n","\n","Epoch 00846: val_loss did not improve from 0.22988\n","Epoch 847/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0266 - val_loss: 0.2312\n","\n","Epoch 00847: val_loss did not improve from 0.22988\n","Epoch 848/2000\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0265 - val_loss: 0.2312\n","\n","Epoch 00848: val_loss did not improve from 0.22988\n","Epoch 849/2000\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0264 - val_loss: 0.2313\n","\n","Epoch 00849: val_loss did not improve from 0.22988\n","Epoch 850/2000\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0263 - val_loss: 0.2314\n","\n","Epoch 00850: val_loss did not improve from 0.22988\n","Epoch 851/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0262 - val_loss: 0.2314\n","\n","Epoch 00851: val_loss did not improve from 0.22988\n","Epoch 852/2000\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0261 - val_loss: 0.2315\n","\n","Epoch 00852: val_loss did not improve from 0.22988\n","Epoch 853/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0260 - val_loss: 0.2316\n","\n","Epoch 00853: val_loss did not improve from 0.22988\n","Epoch 854/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0259 - val_loss: 0.2317\n","\n","Epoch 00854: val_loss did not improve from 0.22988\n","Epoch 855/2000\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0258 - val_loss: 0.2318\n","\n","Epoch 00855: val_loss did not improve from 0.22988\n","Epoch 856/2000\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0257 - val_loss: 0.2318\n","\n","Epoch 00856: val_loss did not improve from 0.22988\n","Epoch 857/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0255 - val_loss: 0.2319\n","\n","Epoch 00857: val_loss did not improve from 0.22988\n","Epoch 858/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0254 - val_loss: 0.2320\n","\n","Epoch 00858: val_loss did not improve from 0.22988\n","Epoch 859/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0253 - val_loss: 0.2320\n","\n","Epoch 00859: val_loss did not improve from 0.22988\n","Epoch 860/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0252 - val_loss: 0.2321\n","\n","Epoch 00860: val_loss did not improve from 0.22988\n","Epoch 861/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0251 - val_loss: 0.2322\n","\n","Epoch 00861: val_loss did not improve from 0.22988\n","Epoch 862/2000\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0250 - val_loss: 0.2323\n","\n","Epoch 00862: val_loss did not improve from 0.22988\n","Epoch 863/2000\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0249 - val_loss: 0.2324\n","\n","Epoch 00863: val_loss did not improve from 0.22988\n","Epoch 864/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0248 - val_loss: 0.2325\n","\n","Epoch 00864: val_loss did not improve from 0.22988\n","Epoch 865/2000\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0247 - val_loss: 0.2326\n","\n","Epoch 00865: val_loss did not improve from 0.22988\n","Epoch 866/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0246 - val_loss: 0.2327\n","\n","Epoch 00866: val_loss did not improve from 0.22988\n","Epoch 867/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0245 - val_loss: 0.2327\n","\n","Epoch 00867: val_loss did not improve from 0.22988\n","Epoch 868/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0244 - val_loss: 0.2328\n","\n","Epoch 00868: val_loss did not improve from 0.22988\n","Epoch 869/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0243 - val_loss: 0.2329\n","\n","Epoch 00869: val_loss did not improve from 0.22988\n","Epoch 870/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0242 - val_loss: 0.2330\n","\n","Epoch 00870: val_loss did not improve from 0.22988\n","Epoch 871/2000\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0241 - val_loss: 0.2331\n","\n","Epoch 00871: val_loss did not improve from 0.22988\n","Epoch 872/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0240 - val_loss: 0.2332\n","\n","Epoch 00872: val_loss did not improve from 0.22988\n","Epoch 873/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0239 - val_loss: 0.2333\n","\n","Epoch 00873: val_loss did not improve from 0.22988\n","Epoch 874/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0238 - val_loss: 0.2334\n","\n","Epoch 00874: val_loss did not improve from 0.22988\n","Epoch 875/2000\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0237 - val_loss: 0.2335\n","\n","Epoch 00875: val_loss did not improve from 0.22988\n","Epoch 876/2000\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0236 - val_loss: 0.2336\n","\n","Epoch 00876: val_loss did not improve from 0.22988\n","Epoch 877/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0235 - val_loss: 0.2337\n","\n","Epoch 00877: val_loss did not improve from 0.22988\n","Epoch 878/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0234 - val_loss: 0.2338\n","\n","Epoch 00878: val_loss did not improve from 0.22988\n","Epoch 879/2000\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0233 - val_loss: 0.2340\n","\n","Epoch 00879: val_loss did not improve from 0.22988\n","Epoch 880/2000\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0232 - val_loss: 0.2341\n","\n","Epoch 00880: val_loss did not improve from 0.22988\n","Epoch 881/2000\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0231 - val_loss: 0.2342\n","\n","Epoch 00881: val_loss did not improve from 0.22988\n","Epoch 882/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0230 - val_loss: 0.2343\n","\n","Epoch 00882: val_loss did not improve from 0.22988\n","Epoch 883/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0229 - val_loss: 0.2344\n","\n","Epoch 00883: val_loss did not improve from 0.22988\n","Epoch 884/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0228 - val_loss: 0.2345\n","\n","Epoch 00884: val_loss did not improve from 0.22988\n","Epoch 885/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0227 - val_loss: 0.2346\n","\n","Epoch 00885: val_loss did not improve from 0.22988\n","Epoch 886/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0226 - val_loss: 0.2348\n","\n","Epoch 00886: val_loss did not improve from 0.22988\n","Epoch 887/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0225 - val_loss: 0.2349\n","\n","Epoch 00887: val_loss did not improve from 0.22988\n","Epoch 888/2000\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0224 - val_loss: 0.2350\n","\n","Epoch 00888: val_loss did not improve from 0.22988\n","Epoch 889/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0223 - val_loss: 0.2351\n","\n","Epoch 00889: val_loss did not improve from 0.22988\n","Epoch 890/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0222 - val_loss: 0.2352\n","\n","Epoch 00890: val_loss did not improve from 0.22988\n","Epoch 891/2000\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0221 - val_loss: 0.2353\n","\n","Epoch 00891: val_loss did not improve from 0.22988\n","Epoch 892/2000\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0220 - val_loss: 0.2354\n","\n","Epoch 00892: val_loss did not improve from 0.22988\n","Epoch 893/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0219 - val_loss: 0.2355\n","\n","Epoch 00893: val_loss did not improve from 0.22988\n","Epoch 894/2000\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0218 - val_loss: 0.2356\n","\n","Epoch 00894: val_loss did not improve from 0.22988\n","Epoch 895/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0218 - val_loss: 0.2357\n","\n","Epoch 00895: val_loss did not improve from 0.22988\n","Epoch 896/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0217 - val_loss: 0.2359\n","\n","Epoch 00896: val_loss did not improve from 0.22988\n","Epoch 897/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0216 - val_loss: 0.2360\n","\n","Epoch 00897: val_loss did not improve from 0.22988\n","Epoch 898/2000\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0215 - val_loss: 0.2361\n","\n","Epoch 00898: val_loss did not improve from 0.22988\n","Epoch 899/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0214 - val_loss: 0.2362\n","\n","Epoch 00899: val_loss did not improve from 0.22988\n","Epoch 900/2000\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0213 - val_loss: 0.2364\n","\n","Epoch 00900: val_loss did not improve from 0.22988\n","Epoch 901/2000\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0212 - val_loss: 0.2365\n","\n","Epoch 00901: val_loss did not improve from 0.22988\n","Epoch 902/2000\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0211 - val_loss: 0.2366\n","\n","Epoch 00902: val_loss did not improve from 0.22988\n","Epoch 903/2000\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0210 - val_loss: 0.2367\n","\n","Epoch 00903: val_loss did not improve from 0.22988\n","Epoch 904/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0209 - val_loss: 0.2368\n","\n","Epoch 00904: val_loss did not improve from 0.22988\n","Epoch 905/2000\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0209 - val_loss: 0.2370\n","\n","Epoch 00905: val_loss did not improve from 0.22988\n","Epoch 906/2000\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0208 - val_loss: 0.2371\n","\n","Epoch 00906: val_loss did not improve from 0.22988\n","Epoch 907/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0207 - val_loss: 0.2372\n","\n","Epoch 00907: val_loss did not improve from 0.22988\n","Epoch 908/2000\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0206 - val_loss: 0.2373\n","\n","Epoch 00908: val_loss did not improve from 0.22988\n","Epoch 00908: early stopping\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"7I6jFxSi9Npm"},"source":["# Load Model\n","# [Reference for loading model](https://machinelearningmastery.com/save-load-keras-deep-learning-models/)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZoH0UHVO9PWO","executionInfo":{"status":"ok","timestamp":1620194919098,"user_tz":-360,"elapsed":1235,"user":{"displayName":"Haisam Rafid","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCFh6znq8o7cMmaOaXWi0DlP9pXtcctfquM4PxnA=s64","userId":"02589669764555967414"}},"outputId":"c0c0cc96-aec8-4ab1-f9dc-b8ba1e28337f"},"source":["import pickle as pkl\n","from keras.models import model_from_json\n","\n","f = open('model.json', 'r')\n","json_model = f.read()\n","\n","model = model_from_json(json_model)\n","model.compile(loss='binary_crossentropy', optimizer='adam')\n","\n","model.load_weights('best_model.h5')\n","\n","model.evaluate(test_x, test_y)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["3/3 [==============================] - 0s 4ms/step - loss: 0.2081\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.22988303005695343"]},"metadata":{"tags":[]},"execution_count":17}]}]}